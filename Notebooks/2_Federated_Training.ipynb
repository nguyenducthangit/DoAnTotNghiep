{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 2. Federated Learning Training\n",
                "\n",
                "**Project:** IoT Network Attack Detection using Federated Learning  \n",
                "**Algorithm:** FedAvg (Federated Averaging)  \n",
                "**Author:** Nguyen Duc Thang\n",
                "\n",
                "---\n",
                "\n",
                "## üìã Objectives\n",
                "\n",
                "1. Load preprocessed client data\n",
                "2. Create global DNN model\n",
                "3. Initialize Federated Server and Clients\n",
                "4. Run FL training loop (30-50 rounds)\n",
                "   - Server broadcasts model ‚Üí Clients train locally ‚Üí Server aggregates (FedAvg)\n",
                "5. Save trained model and training history\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ Expected Outputs\n",
                "\n",
                "- `../Output/models/global_model.h5`\n",
                "- `../Output/metrics/training_history.json`\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "2025-12-30 17:30:28.865621: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
                        "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚ö†Ô∏è  No GPU found. Training will use CPU (slower).\n",
                        "\n",
                        "‚úÖ TensorFlow version: 2.11.0\n",
                        "‚úÖ Keras version: 2.11.0\n"
                    ]
                }
            ],
            "source": [
                "# Standard libraries\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import json\n",
                "import yaml\n",
                "import matplotlib.pyplot as plt\n",
                "from datetime import datetime\n",
                "\n",
                "# Framework-agnostic imports\n",
                "import torch  # PyTorch (only used if framework='pytorch')\n",
                "\n",
                "# TensorFlow/Keras\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "# Import our utility modules\n",
                "from utils import data_utils, model_utils, fl_utils\n",
                "from utils.fedmade_aggregation import fedmade_aggregate_with_fallback\n",
                "\n",
                "# Set random seeds for reproducibility\n",
                "np.random.seed(42)\n",
                "tf.random.set_seed(42)\n",
                "\n",
                "# Check GPU availability\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    print(f\"‚úÖ GPU available: {len(gpus)} device(s)\")\n",
                "    for gpu in gpus:\n",
                "        print(f\"   {gpu}\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  No GPU found. Training will use CPU (slower).\")\n",
                "\n",
                "print(f\"\\n‚úÖ TensorFlow version: {tf.__version__}\")\n",
                "print(f\"‚úÖ Keras version: {keras.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Configuration\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÑ Configuration loaded:\n",
                        "\n",
                        "üîß FL Settings:\n",
                        "   Number of clients: 5\n",
                        "   Number of rounds: 30\n",
                        "   Local epochs: 10\n",
                        "   Batch size: 256\n",
                        "\n",
                        "üèóÔ∏è  Model Architecture:\n",
                        "   Input dim: 46\n",
                        "   Hidden layers: [128, 64, 32]\n",
                        "   Output classes: 34\n",
                        "   Dropout rate: 0.3\n",
                        "\n",
                        "‚öôÔ∏è  Optimizer:\n",
                        "   Type: adam\n",
                        "   Learning rate: 0.001\n"
                    ]
                }
            ],
            "source": [
                "# Load training configuration\n",
                "config_path = 'configs/training_config.yaml'\n",
                "\n",
                "with open(config_path, 'r') as f:\n",
                "    config = yaml.safe_load(f)\n",
                "\n",
                "print(\"üìÑ Configuration loaded:\")\n",
                "print(f\"\\nüîß FL Settings:\")\n",
                "print(f\"   Number of clients: {config['num_clients']}\")\n",
                "print(f\"   Number of rounds: {config['num_rounds']}\")\n",
                "print(f\"   Local epochs: {config['local_epochs']}\")\n",
                "print(f\"   Batch size: {config['batch_size']}\")\n",
                "\n",
                "print(f\"\\nüèóÔ∏è  Model Architecture:\")\n",
                "print(f\"   Input dim: {config['model']['input_dim']}\")\n",
                "print(f\"   Hidden layers: {config['model']['hidden_layers']}\")\n",
                "print(f\"   Output classes: {config['model']['num_classes']}\")\n",
                "print(f\"   Dropout rate: {config['model']['dropout_rate']}\")\n",
                "\n",
                "print(f\"\\n‚öôÔ∏è  Optimizer:\")\n",
                "print(f\"   Type: {config['optimizer']['type']}\")\n",
                "print(f\"   Learning rate: {config['optimizer']['learning_rate']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "================================================================================\n",
                        "CHI·∫æN L∆Ø·ª¢C AGGREGATION: FEDMADE\n",
                        "================================================================================\n",
                        "\n",
                        "‚öôÔ∏è  C·∫•u h√¨nh FedMade:\n",
                        "   Contribution threshold: 0.2\n",
                        "   Accuracy weight: 0.7\n",
                        "   Loss weight: 0.3\n",
                        "   Verbose logging: True\n",
                        "\n",
                        "üí° FedMade s·∫Ω t√≠nh ƒëi·ªÉm ƒë√≥ng g√≥p ƒë·ªông d·ª±a tr√™n performance c·ªßa clients\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# LOAD AGGREGATION CONFIGURATION\n",
                "# ============================================================================\n",
                "\n",
                "agg_method = config['aggregation']['method']\n",
                "fedmade_config = config['aggregation']['fedmade']\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(f\"CHI·∫æN L∆Ø·ª¢C AGGREGATION: {agg_method.upper()}\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "if agg_method == 'fedmade':\n",
                "    print(f\"\\n‚öôÔ∏è  C·∫•u h√¨nh FedMade:\")\n",
                "    print(f\"   Contribution threshold: {fedmade_config['contribution_threshold']}\")\n",
                "    print(f\"   Accuracy weight: {fedmade_config['accuracy_weight']}\")\n",
                "    print(f\"   Loss weight: {fedmade_config['loss_weight']}\")\n",
                "    print(f\"   Verbose logging: {fedmade_config['verbose']}\")\n",
                "    print(f\"\\nüí° FedMade s·∫Ω t√≠nh ƒëi·ªÉm ƒë√≥ng g√≥p ƒë·ªông d·ª±a tr√™n performance c·ªßa clients\")\n",
                "else:\n",
                "    print(f\"\\nüìä S·ª≠ d·ª•ng FedAvg (trung b√¨nh c·ªông chu·∫©n)\")\n",
                "\n",
                "# Kh·ªüi t·∫°o l∆∞u tr·ªØ contribution scores\n",
                "contribution_scores_history = [] if agg_method == 'fedmade' else None\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Preprocessed Data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üìÇ Loading client data...\n",
                        "\n",
                        "   ‚úì client_0: 1,193,393 samples\n",
                        "   ‚úì client_1: 197,234 samples\n",
                        "   ‚úì client_2: 323,594 samples\n",
                        "   ‚úì client_3: 215,564 samples\n",
                        "   ‚úì client_4: 164,893 samples\n",
                        "   ‚úì test: 523,670 samples\n",
                        "\n",
                        "‚úÖ All data loaded successfully!\n"
                    ]
                }
            ],
            "source": [
                "# Load client data\n",
                "data_dir = '../Output/data'\n",
                "\n",
                "print(\"üìÇ Loading client data...\\n\")\n",
                "\n",
                "# Load data for each client\n",
                "client_datasets = {}\n",
                "for i in range(config['num_clients']):\n",
                "    client_name = f'client_{i}'\n",
                "    data = data_utils.load_client_data(data_dir, client_name)\n",
                "    client_datasets[client_name] = data\n",
                "    print(f\"   ‚úì {client_name}: {len(data['X']):,} samples\")\n",
                "\n",
                "# Load test data\n",
                "test_data = data_utils.load_client_data(data_dir, 'test')\n",
                "X_test = test_data['X']\n",
                "y_test = test_data['y']\n",
                "print(f\"   ‚úì test: {len(X_test):,} samples\")\n",
                "\n",
                "print(f\"\\n‚úÖ All data loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Create Global Model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîß Selected Framework: PYTORCH\n",
                        "================================================================================\n",
                        "\n",
                        "üèóÔ∏è  Creating PyTorch TabTransformer model...\n",
                        "\n",
                        "‚ö†Ô∏è  Feature config not found at: ../Output/models/feature_config.json\n",
                        "   Creating a default feature_config.json (safe fallback)...\n",
                        "   ‚úì Saved: ../Output/models/feature_config.json\n",
                        "üìä Feature Configuration:\n",
                        "   Categorical features: 20\n",
                        "   Numerical features: 26\n",
                        "   Total features: 46\n",
                        "\n",
                        "\n",
                        "üèóÔ∏è  Created TabTransformer model:\n",
                        "   Categorical features: 20\n",
                        "   Numerical features: 26\n",
                        "   Total features: 46\n",
                        "   Embedding dimension: 32\n",
                        "   Transformer layers: 2\n",
                        "   Attention heads: 4\n",
                        "   Output classes: 34\n",
                        "   Total parameters: 66,178\n",
                        "   Trainable parameters: 66,178\n",
                        "   Model size: 258.51 KB (FP32)\n",
                        "\n",
                        "üíª Device: cpu\n",
                        "   ‚ö†Ô∏è  Using CPU. Training will be slower.\n",
                        "\n",
                        "================================================================================\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# CREATE GLOBAL MODEL (Framework-Agnostic)\n",
                "# ============================================================================\n",
                "\n",
                "# Get framework from config\n",
                "framework = config.get('framework', 'tensorflow')\n",
                "\n",
                "print(f\"üîß Selected Framework: {framework.upper()}\")\n",
                "print(f\"{'='*80}\\n\")\n",
                "\n",
                "if framework == 'pytorch':\n",
                "    # ========== PYTORCH TABTRANSFORMER PATH ==========\n",
                "    print(\"üèóÔ∏è  Creating PyTorch TabTransformer model...\\n\")\n",
                "\n",
                "    from utils.model_utils_pytorch import create_tabtransformer_from_config\n",
                "    import os\n",
                "    import json\n",
                "\n",
                "    # Load (or create) feature configuration\n",
                "    feature_config_path = '../Output/models/feature_config.json'\n",
                "\n",
                "    if os.path.exists(feature_config_path):\n",
                "        with open(feature_config_path, 'r') as f:\n",
                "            feature_config = json.load(f)\n",
                "    else:\n",
                "        print(f\"‚ö†Ô∏è  Feature config not found at: {feature_config_path}\")\n",
                "        print(\"   Creating a default feature_config.json (safe fallback)...\")\n",
                "\n",
                "        feature_config = dict(config.get('features', {}) or {})\n",
                "        input_dim = int(config.get('model', {}).get('input_dim', 46))\n",
                "\n",
                "        # If not provided, use the project default split used elsewhere in the code:\n",
                "        # first 20 columns are treated as categorical (see utils/fl_utils_pytorch.split_features)\n",
                "        if not feature_config.get('categorical_cardinalities'):\n",
                "            num_categorical = min(20, input_dim)\n",
                "            feature_config['categorical_indices'] = list(range(num_categorical))\n",
                "            feature_config['numerical_indices'] = list(range(num_categorical, input_dim))\n",
                "            feature_config['categorical_cardinalities'] = [50] * num_categorical\n",
                "\n",
                "        feature_config['num_categorical'] = len(feature_config.get('categorical_cardinalities', []))\n",
                "        feature_config['num_numerical'] = input_dim - feature_config['num_categorical']\n",
                "        feature_config['total_features'] = input_dim\n",
                "\n",
                "        os.makedirs(os.path.dirname(feature_config_path), exist_ok=True)\n",
                "        with open(feature_config_path, 'w') as f:\n",
                "            json.dump(feature_config, f, indent=2)\n",
                "\n",
                "        print(f\"   ‚úì Saved: {feature_config_path}\")\n",
                "\n",
                "    print(f\"üìä Feature Configuration:\")\n",
                "    print(f\"   Categorical features: {feature_config['num_categorical']}\")\n",
                "    print(f\"   Numerical features: {feature_config['num_numerical']}\")\n",
                "    print(f\"   Total features: {feature_config['total_features']}\\n\")\n",
                "\n",
                "    # Add feature config to model config\n",
                "    config['features'] = feature_config\n",
                "\n",
                "    # Create TabTransformer\n",
                "    global_model = create_tabtransformer_from_config(config)\n",
                "    \n",
                "    # Set device\n",
                "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "    global_model.to(device)\n",
                "    print(f\"\\nüíª Device: {device}\")\n",
                "    \n",
                "    if device == 'cpu':\n",
                "        print(\"   ‚ö†Ô∏è  Using CPU. Training will be slower.\")\n",
                "    \n",
                "else:\n",
                "    # ========== TENSORFLOW DNN PATH (EXISTING) ==========\n",
                "    print(\"üèóÔ∏è  Creating TensorFlow DNN model...\\n\")\n",
                "    \n",
                "    # Create and compile global model (existing code)\n",
                "    global_model = model_utils.create_and_compile_model(config)\n",
                "    \n",
                "    # Print model summary (existing code)\n",
                "    model_utils.print_model_summary(global_model)\n",
                "\n",
                "print(f\"\\n{'='*80}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Initialize Federated Server and Clients\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üì° Creating PyTorch DataLoaders for clients...\n",
                        "   ‚úì client_0: 1,193,393 samples\n",
                        "   ‚úì client_1: 197,234 samples\n",
                        "   ‚úì client_2: 323,594 samples\n",
                        "   ‚úì client_3: 215,564 samples\n",
                        "   ‚úì client_4: 164,893 samples\n",
                        "   ‚úì test: 523,670 samples\n",
                        "\n",
                        "‚úÖ 5 PyTorch DataLoaders created!\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# INITIALIZE FEDERATED SERVER AND CLIENTS (Framework-Agnostic)\n",
                "# ============================================================================\n",
                "\n",
                "if framework == 'pytorch':\n",
                "    # ========== PYTORCH PATH ==========\n",
                "    from utils.fl_utils_pytorch import create_data_loaders\n",
                "    \n",
                "    print(\"üì° Creating PyTorch DataLoaders for clients...\")\n",
                "    \n",
                "    # Create DataLoaders for each client\n",
                "    client_loaders = []\n",
                "    for i in range(config['num_clients']):\n",
                "        client_name = f'client_{i}'\n",
                "        client_data = client_datasets[client_name]\n",
                "        \n",
                "        loader = create_data_loaders(\n",
                "            X=client_data['X'],\n",
                "            y=client_data['y'],\n",
                "            batch_size=config['batch_size'],\n",
                "            shuffle=True\n",
                "        )\n",
                "        client_loaders.append(loader)\n",
                "        print(f\"   ‚úì {client_name}: {len(client_data['X']):,} samples\")\n",
                "    \n",
                "    # Create test DataLoader\n",
                "    test_loader = create_data_loaders(\n",
                "        X=X_test,\n",
                "        y=y_test,\n",
                "        batch_size=config['batch_size'],\n",
                "        shuffle=False\n",
                "    )\n",
                "    print(f\"   ‚úì test: {len(X_test):,} samples\")\n",
                "    \n",
                "    print(f\"\\n‚úÖ {len(client_loaders)} PyTorch DataLoaders created!\")\n",
                "    \n",
                "else:\n",
                "    # ========== TENSORFLOW PATH (EXISTING) ==========\n",
                "    \n",
                "    # Initialize Federated Server\n",
                "    print(\"üñ•Ô∏è  Initializing Federated Server...\")\n",
                "    server = fl_utils.FederatedServer(model=global_model)\n",
                "    print(\"   ‚úì Server initialized\\n\")\n",
                "    \n",
                "    # Initialize Federated Clients\n",
                "    print(\"üë• Initializing Federated Clients...\")\n",
                "    clients = []\n",
                "    for i in range(config['num_clients']):\n",
                "        client_name = f'client_{i}'\n",
                "        client_data = client_datasets[client_name]\n",
                "        \n",
                "        client = fl_utils.FederatedClient(\n",
                "            client_id=i,\n",
                "            X_train=client_data['X'],\n",
                "            y_train=client_data['y']\n",
                "        )\n",
                "        clients.append(client)\n",
                "    \n",
                "    print(f\"\\n‚úÖ {len(clients)} clients initialized!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Run Federated Learning Training\n",
                "\n",
                "This is the main training loop. It will take several hours depending on:\n",
                "\n",
                "- Dataset size\n",
                "- Number of rounds\n",
                "- Hardware (GPU vs CPU)\n",
                "\n",
                "**Estimated time:**\n",
                "\n",
                "- With GPU: 4-6 hours (full dataset, 30 rounds)\n",
                "- With CPU: 8-12 hours (full dataset, 30 rounds)\n",
                "- With 10% sample: 30-60 minutes\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üïê Training started at: 2025-12-30 17:31:03\n",
                        "\n",
                        "\n",
                        "================================================================================\n",
                        "FEDERATED LEARNING TRAINING (PyTorch + TabTransformer)\n",
                        "================================================================================\n",
                        "Number of clients: 5\n",
                        "Number of rounds: 30\n",
                        "Local epochs per round: 10\n",
                        "Learning rate: 0.001\n",
                        "Device: cpu\n",
                        "================================================================================\n",
                        "\n",
                        "\n",
                        "================================================================================\n",
                        "ROUND 1/30\n",
                        "================================================================================\n",
                        "üì° Distributing global model to 5 clients...\n",
                        "\n",
                        "   Client 0 training...\n"
                    ]
                }
            ],
            "source": [
                "# ============================================================================\n",
                "# RUN FEDERATED LEARNING TRAINING (Framework-Agnostic)\n",
                "# ============================================================================\n",
                "\n",
                "# Record start time\n",
                "start_time = datetime.now()\n",
                "print(f\"üïê Training started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
                "\n",
                "if framework == 'pytorch':\n",
                "    # ========== PYTORCH TRAINING ==========\n",
                "    from utils.fl_utils_pytorch import federated_training_loop_pytorch\n",
                "    \n",
                "    history = federated_training_loop_pytorch(\n",
                "        global_model=global_model,\n",
                "        client_data_loaders=client_loaders,\n",
                "        test_loader=test_loader,\n",
                "        num_rounds=config['num_rounds'],\n",
                "        local_epochs=config['local_epochs'],\n",
                "        learning_rate=config['optimizer']['learning_rate'],\n",
                "        device=device,\n",
                "        num_categorical=feature_config['num_categorical'],\n",
                "        categorical_cardinalities=feature_config['categorical_cardinalities'],\n",
                "        verbose=True,\n",
                "        aggregation_method=agg_method,\n",
                "        aggregation_config=fedmade_config if agg_method == 'fedmade' else None,\n",
                "        client_metrics_history=contribution_scores_history\n",
                "    )\n",
                "    \n",
                "    # Convert to same format as TensorFlow history\n",
                "    training_history = {\n",
                "        'round': history['round'],\n",
                "        'accuracy': history['accuracy'],\n",
                "        'loss': history['loss']\n",
                "    }\n",
                "    \n",
                "else:\n",
                "    # ========== TENSORFLOW TRAINING (EXISTING) ==========\n",
                "    \n",
                "    # Run federated training loop\n",
                "    training_history = fl_utils.federated_training_loop(\n",
                "        server=server,\n",
                "        clients=clients,\n",
                "        X_test=X_test,\n",
                "        y_test=y_test,\n",
                "        num_rounds=config['num_rounds'],\n",
                "        local_epochs=config['local_epochs'],\n",
                "        batch_size=config['batch_size'],\n",
                "        verbose=1\n",
                "    )\n",
                "\n",
                "# Record end time  \n",
                "end_time = datetime.now()\n",
                "training_duration = end_time - start_time\n",
                "\n",
                "print(f\"\\nüïê Training completed at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
                "print(f\"‚è±Ô∏è  Total training time: {training_duration}\")\n",
                "print(f\"   ({training_duration.total_seconds() / 60:.2f} minutes)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visualize Training Progress\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training curves\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Plot accuracy\n",
                "axes[0].plot(training_history['round'], training_history['accuracy'], \n",
                "            marker='o', linewidth=2, markersize=6)\n",
                "axes[0].set_title('Global Model Accuracy vs Round', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Round', fontsize=12)\n",
                "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[0].grid(True, alpha=0.3)\n",
                "axes[0].set_ylim([0, 1])\n",
                "\n",
                "# Add horizontal line at 95% target\n",
                "axes[0].axhline(y=0.95, color='r', linestyle='--', linewidth=2, label='Target (95%)')\n",
                "axes[0].legend()\n",
                "\n",
                "# Plot loss\n",
                "axes[1].plot(training_history['round'], training_history['loss'], \n",
                "            marker='o', linewidth=2, markersize=6, color='orange')\n",
                "axes[1].set_title('Global Model Loss vs Round', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Round', fontsize=12)\n",
                "axes[1].set_ylabel('Loss', fontsize=12)\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Print final metrics\n",
                "final_accuracy = training_history['accuracy'][-1]\n",
                "final_loss = training_history['loss'][-1]\n",
                "\n",
                "print(f\"\\nüìä Final Metrics:\")\n",
                "print(f\"   Accuracy: {final_accuracy:.4f} ({final_accuracy*100:.2f}%)\")\n",
                "print(f\"   Loss: {final_loss:.4f}\")\n",
                "\n",
                "if final_accuracy >= 0.95:\n",
                "    print(f\"\\n‚úÖ Target accuracy (>95%) achieved!\")\n",
                "else:\n",
                "    print(f\"\\n‚ö†Ô∏è  Target accuracy (>95%) not yet achieved.\")\n",
                "    print(f\"   Consider increasing num_rounds or tuning hyperparameters.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Trained Model\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# SAVE TRAINED MODEL (Framework-Agnostic)\n",
                "# ============================================================================\n",
                "\n",
                "# Create models directory if it doesn't exist\n",
                "models_dir = '../Output/models'\n",
                "os.makedirs(models_dir, exist_ok=True)\n",
                "\n",
                "if framework == 'pytorch':\n",
                "    # ========== SAVE PYTORCH MODEL ==========\n",
                "    model_path = os.path.join(models_dir, 'global_model.pth')\n",
                "    \n",
                "    # Save model state dict\n",
                "    torch.save(global_model.state_dict(), model_path)\n",
                "    \n",
                "    print(f\"üíæ PyTorch model saved to: {model_path}\")\n",
                "    \n",
                "    # Get model size\n",
                "    model_size_mb = os.path.getsize(model_path) / (1024 ** 2)\n",
                "    \n",
                "else:\n",
                "    # ========== SAVE TENSORFLOW MODEL (EXISTING) ==========\n",
                "    model_path = os.path.join(models_dir, config['paths']['global_model'])\n",
                "    \n",
                "    # Save model\n",
                "    server.model.save(model_path)\n",
                "    print(f\"   üíæ Global model saved to: {model_path}\")\n",
                "    \n",
                "    # Get model size\n",
                "    model_size_mb = os.path.getsize(model_path) / (1024 ** 2)\n",
                "\n",
                "print(f\"\\n‚úÖ Model saved successfully!\")\n",
                "print(f\"   Path: {model_path}\")\n",
                "print(f\"   Size: {model_size_mb:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# L∆ØU FEDMADE METRICS\n",
                "# ============================================================================\n",
                "\n",
                "if agg_method == 'fedmade' and contribution_scores_history:\n",
                "    import json\n",
                "    from datetime import datetime\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*80)\n",
                "    print(\"L∆ØU FEDMADE METRICS\")\n",
                "    print(\"=\"*80)\n",
                "    \n",
                "    metrics_dir = fedmade_config['output_dir']\n",
                "    os.makedirs(metrics_dir, exist_ok=True)\n",
                "    \n",
                "    # L∆∞u contribution scores history\n",
                "    scores_path = os.path.join(metrics_dir, fedmade_config['contribution_scores_json'])\n",
                "    with open(scores_path, 'w') as f:\n",
                "        json.dump(contribution_scores_history, f, indent=2)\n",
                "    \n",
                "    print(f\"\\nüíæ FedMade metrics ƒë√£ l∆∞u:\")\n",
                "    print(f\"   - Contribution scores: {scores_path}\")\n",
                "    print(f\"   - T·ªïng s·ªë rounds: {len(contribution_scores_history)}\")\n",
                "    \n",
                "    print(f\"\\n‚úÖ C√≥ th·ªÉ s·ª≠ d·ª•ng scores n√†y ƒë·ªÉ v·∫Ω heatmap contribution\")\n",
                "else:\n",
                "    print(\"\\n‚è≠Ô∏è  B·ªè qua l∆∞u FedMade metrics (kh√¥ng s·ª≠ d·ª•ng FedMade)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Training History\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare training history for saving\n",
                "history_to_save = {\n",
                "    'config': {\n",
                "        'num_clients': config['num_clients'],\n",
                "        'num_rounds': config['num_rounds'],\n",
                "        'local_epochs': config['local_epochs'],\n",
                "        'batch_size': config['batch_size'],\n",
                "        'learning_rate': config['optimizer']['learning_rate'],\n",
                "        'model_architecture': config['model']['hidden_layers']\n",
                "    },\n",
                "    'training_info': {\n",
                "        'start_time': start_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
                "        'end_time': end_time.strftime('%Y-%m-%d %H:%M:%S'),\n",
                "        'duration_seconds': training_duration.total_seconds(),\n",
                "        'gpu_used': len(gpus) > 0\n",
                "    },\n",
                "    'history': {\n",
                "        'round': training_history['round'],\n",
                "        'loss': [float(x) for x in training_history['loss']],  # Convert to float for JSON\n",
                "        'accuracy': [float(x) for x in training_history['accuracy']]\n",
                "    },\n",
                "    'final_metrics': {\n",
                "        'accuracy': float(final_accuracy),\n",
                "        'loss': float(final_loss)\n",
                "    }\n",
                "}\n",
                "\n",
                "# Save to JSON\n",
                "output_metrics_dir = '../Output/metrics'\n",
                "os.makedirs(output_metrics_dir, exist_ok=True)\n",
                "\n",
                "history_path = os.path.join(output_metrics_dir, config['paths']['training_history'])\n",
                "with open(history_path, 'w') as f:\n",
                "    json.dump(history_to_save, f, indent=2)\n",
                "\n",
                "print(f\"üíæ Training history saved to: {history_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Quick Evaluation on Test Set\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the saved model and evaluate\n",
                "print(\"üîç Loading saved model for verification...\")\n",
                "loaded_model = model_utils.load_model(model_path)\n",
                "\n",
                "# Evaluate on test set\n",
                "print(\"\\nüìä Evaluating on test set...\")\n",
                "test_loss, test_accuracy = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
                "\n",
                "print(f\"\\n‚úÖ Test Set Results:\")\n",
                "print(f\"   Loss: {test_loss:.4f}\")\n",
                "print(f\"   Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
                "\n",
                "# Generate predictions for a few samples\n",
                "print(f\"\\nüîÆ Sample predictions (first 10 test samples):\")\n",
                "sample_predictions = loaded_model.predict(X_test[:10], verbose=0)\n",
                "predicted_classes = np.argmax(sample_predictions, axis=1)\n",
                "true_classes = y_test[:10]\n",
                "\n",
                "for i in range(10):\n",
                "    match = \"‚úì\" if predicted_classes[i] == true_classes[i] else \"‚úó\"\n",
                "    print(f\"   {match} Sample {i+1}: Predicted={predicted_classes[i]}, True={true_classes[i]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Summary\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"FEDERATED LEARNING TRAINING SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nüîß Training Configuration:\")\n",
                "print(f\"   Clients: {config['num_clients']}\")\n",
                "print(f\"   Rounds: {config['num_rounds']}\")\n",
                "print(f\"   Local epochs: {config['local_epochs']}\")\n",
                "print(f\"   Batch size: {config['batch_size']}\")\n",
                "print(f\"   Learning rate: {config['optimizer']['learning_rate']}\")\n",
                "\n",
                "print(f\"\\n‚è±Ô∏è  Training Time:\")\n",
                "print(f\"   Duration: {training_duration}\")\n",
                "print(f\"   ({training_duration.total_seconds() / 60:.2f} minutes)\")\n",
                "print(f\"   ({training_duration.total_seconds() / 3600:.2f} hours)\")\n",
                "\n",
                "print(f\"\\nüìä Final Results:\")\n",
                "print(f\"   Test Accuracy: {test_accuracy*100:.2f}%\")\n",
                "print(f\"   Test Loss: {test_loss:.4f}\")\n",
                "\n",
                "if test_accuracy >= 0.95:\n",
                "    print(f\"\\n‚úÖ SUCCESS: Target accuracy (>95%) achieved!\")\n",
                "else:\n",
                "    print(f\"\\n‚ö†Ô∏è  Target accuracy (>95%) not achieved.\")\n",
                "    print(f\"   Gap: {(0.95 - test_accuracy)*100:.2f}%\")\n",
                "    print(f\"\\nüí° Suggestions:\")\n",
                "    print(f\"   - Increase num_rounds to 50\")\n",
                "    print(f\"   - Reduce learning_rate to 0.0005\")\n",
                "    print(f\"   - Increase local_epochs to 7\")\n",
                "\n",
                "print(f\"\\nüíæ Output Files:\")\n",
                "print(f\"   Model: {model_path}\")\n",
                "print(f\"   History: {history_path}\")\n",
                "\n",
                "print(f\"\\nüìù Next step: Run 3_Model_Evaluation_Export.ipynb\")\n",
                "print(f\"   for detailed metrics and visualizations.\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
