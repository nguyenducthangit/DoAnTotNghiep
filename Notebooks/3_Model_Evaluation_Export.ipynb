{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Model Evaluation and Export\n",
                "\n",
                "**Project:** IoT Network Attack Detection using Federated Learning  \n",
                "**Author:** Nguyen Duc Thang\n",
                "\n",
                "---\n",
                "\n",
                "## üìã Objectives\n",
                "\n",
                "1. Load trained global model\n",
                "2. Generate predictions on test set\n",
                "3. Calculate comprehensive metrics:\n",
                "   - Overall accuracy\n",
                "   - Per-class Precision, Recall, F1-Score\n",
                "   - Confusion Matrix\n",
                "4. Create visualizations for thesis report\n",
                "5. Export all metrics and artifacts\n",
                "\n",
                "---\n",
                "\n",
                "## üéØ Expected Outputs\n",
                "\n",
                "- `../Output/metrics/confusion_matrix.png`\n",
                "- `../Output/metrics/accuracy_plot.png`\n",
                "- `../Output/metrics/f1_scores_per_class.png`\n",
                "- `../Output/metrics/metrics_report.json`\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Standard libraries\n",
                "import os\n",
                "import sys\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import json\n",
                "import pickle\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix, classification_report, \n",
                "    accuracy_score, precision_recall_fscore_support\n",
                ")\n",
                "\n",
                "# TensorFlow/Keras\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "\n",
                "# Import our utility modules\n",
                "from utils import data_utils, model_utils\n",
                "\n",
                "# Set style for plots\n",
                "sns.set_style('whitegrid')\n",
                "plt.rcParams['figure.dpi'] = 100\n",
                "plt.rcParams['savefig.dpi'] = 300  # High resolution for thesis\n",
                "\n",
                "print(\"‚úÖ All imports successful!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Trained Model and Test Data\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# LOAD TRAINED MODEL (Framework-Agnostic)\n",
                "# ============================================================================\n",
                "\n",
                "import yaml\n",
                "import json\n",
                "\n",
                "# Load config to determine framework\n",
                "config_path = 'configs/training_config.yaml'\n",
                "with open(config_path, 'r') as f:\n",
                "    config = yaml.safe_load(f)\n",
                "\n",
                "framework = config.get('framework', 'tensorflow')\n",
                "\n",
                "print(f\"üîß Framework: {framework.upper()}\")\n",
                "print(f\"{'='*80}\\n\")\n",
                "\n",
                "if framework == 'pytorch':\n",
                "    # ========== LOAD PYTORCH MODEL ==========\n",
                "    import torch\n",
                "    from utils.model_utils_pytorch import create_tabtransformer_from_config\n",
                "    \n",
                "    # Load feature config\n",
                "    feature_config_path = '../Output/models/feature_config.json'\n",
                "    with open(feature_config_path, 'r') as f:\n",
                "        feature_config = json.load(f)\n",
                "    \n",
                "    config['features'] = feature_config\n",
                "    \n",
                "    print(\"üìÇ Loading PyTorch TabTransformer model...\")\n",
                "    \n",
                "    # Create model architecture\n",
                "    model = create_tabtransformer_from_config(config)\n",
                "    \n",
                "    # Load trained weights\n",
                "    model_path = '../Output/models/global_model.pth'\n",
                "    model.load_state_dict(torch.load(model_path, map_location='cpu'))\n",
                "    \n",
                "    # Set to evaluation mode\n",
                "    model.eval()\n",
                "    \n",
                "    # Set device\n",
                "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "    model.to(device)\n",
                "    \n",
                "    print(f\"   ‚úì Model loaded from: {model_path}\")\n",
                "    print(f\"   ‚úì Device: {device}\")\n",
                "    \n",
                "else:\n",
                "    # ========== LOAD TENSORFLOW MODEL (EXISTING) ==========\n",
                "    from tensorflow import keras\n",
                "    \n",
                "    print(\"üìÇ Loading TensorFlow DNN model...\")\n",
                "    \n",
                "    model_path = '../Output/models/global_model.h5'\n",
                "    model = keras.models.load_model(model_path)\n",
                "    \n",
                "    print(f\"   ‚úì Model loaded from: {model_path}\")\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "\n",
                "# Load test data\n",
                "data_dir = '../Output/data'\n",
                "test_data = data_utils.load_client_data(data_dir, 'test')\n",
                "X_test = test_data['X']\n",
                "y_test = test_data['y']\n",
                "\n",
                "print(f\"\\n‚úÖ Data loaded:\")\n",
                "print(f\"   Test samples: {len(X_test):,}\")\n",
                "print(f\"   Features: {X_test.shape[1]}\")\n",
                "print(f\"   Classes: {len(np.unique(y_test))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load Label Mapping\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load label mapping\n",
                "labels_path = '../Output/models/labels.json'\n",
                "with open(labels_path, 'r') as f:\n",
                "    label_mapping = json.load(f)\n",
                "\n",
                "# Convert keys to integers\n",
                "label_mapping = {int(k): v for k, v in label_mapping.items()}\n",
                "\n",
                "print(f\"üìã Label mapping loaded ({len(label_mapping)} classes):\")\n",
                "for i in range(min(10, len(label_mapping))):\n",
                "    print(f\"   {i}: {label_mapping[i]}\")\n",
                "if len(label_mapping) > 10:\n",
                "    print(f\"   ... and {len(label_mapping) - 10} more\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Generate Predictions\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================================\n",
                "# GENERATE PREDICTIONS (Framework-Agnostic)\n",
                "# ============================================================================\n",
                "\n",
                "print(\"üîÆ Generating predictions on test set...\\n\")\n",
                "\n",
                "if framework == 'pytorch':\n",
                "    # ========== PYTORCH PREDICTIONS ==========\n",
                "    import torch\n",
                "    from utils.fl_utils_pytorch import split_features\n",
                "    \n",
                "    # Convert to tensor\n",
                "    X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
                "    \n",
                "    # Number of categorical features\n",
                "    num_categorical = feature_config['num_categorical']\n",
                "    \n",
                "    # Generate predictions\n",
                "    with torch.no_grad():\n",
                "        # Split features\n",
                "        cat_features, num_features = split_features(\n",
                "            X_test_tensor.cpu().numpy(),\n",
                "            num_categorical,\n",
                "            feature_config['categorical_cardinalities']\n",
                "        )\n",
                "        \n",
                "        # Convert to tensors\n",
                "        cat_features = torch.LongTensor(cat_features).to(device)\n",
                "        num_features = torch.FloatTensor(num_features).to(device)\n",
                "        \n",
                "        # Get predictions\n",
                "        logits = model(cat_features, num_features)\n",
                "        y_pred = logits.argmax(dim=1).cpu().numpy()\n",
                "    \n",
                "    print(f\"   ‚úì Generated {len(y_pred):,} predictions using PyTorch\")\n",
                "    \n",
                "else:\n",
                "    # ========== TENSORFLOW PREDICTIONS (EXISTING) ==========\n",
                "    y_pred_proba = model.predict(X_test, verbose=1)\n",
                "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
                "    \n",
                "    print(f\"   ‚úì Generated {len(y_pred):,} predictions using TensorFlow\")\n",
                "\n",
                "print(f\"\\n‚úÖ Predictions complete!\")\n",
                "print(f\"   Test samples: {len(y_pred):,}\")\n",
                "print(f\"   Unique predictions: {len(np.unique(y_pred))}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Calculate Overall Metrics\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate overall accuracy\n",
                "overall_accuracy = accuracy_score(y_test, y_pred)\n",
                "\n",
                "# Calculate per-class metrics\n",
                "precision, recall, f1, support = precision_recall_fscore_support(\n",
                "    y_test, y_pred, average=None, zero_division=0\n",
                ")\n",
                "\n",
                "# Calculate macro and weighted averages\n",
                "precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
                "    y_test, y_pred, average='macro', zero_division=0\n",
                ")\n",
                "\n",
                "precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
                "    y_test, y_pred, average='weighted', zero_division=0\n",
                ")\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"OVERALL METRICS\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\nüìä Overall Accuracy: {overall_accuracy:.4f} ({overall_accuracy*100:.2f}%)\")\n",
                "print(f\"\\nüìà Macro Averages (unweighted):\")\n",
                "print(f\"   Precision: {precision_macro:.4f}\")\n",
                "print(f\"   Recall: {recall_macro:.4f}\")\n",
                "print(f\"   F1-Score: {f1_macro:.4f}\")\n",
                "print(f\"\\nüìà Weighted Averages (by support):\")\n",
                "print(f\"   Precision: {precision_weighted:.4f}\")\n",
                "print(f\"   Recall: {recall_weighted:.4f}\")\n",
                "print(f\"   F1-Score: {f1_weighted:.4f}\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "# Check if target met\n",
                "if overall_accuracy >= 0.95:\n",
                "    print(f\"\\n‚úÖ SUCCESS: Target accuracy (>95%) achieved!\")\n",
                "else:\n",
                "    print(f\"\\n‚ö†Ô∏è  Target accuracy (>95%) not achieved.\")\n",
                "    print(f\"   Gap: {(0.95 - overall_accuracy)*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Per-Class Metrics\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create per-class metrics DataFrame\n",
                "metrics_df = pd.DataFrame({\n",
                "    'Class': [label_mapping[i] for i in range(len(precision))],\n",
                "    'Precision': precision,\n",
                "    'Recall': recall,\n",
                "    'F1-Score': f1,\n",
                "    'Support': support\n",
                "})\n",
                "\n",
                "# Sort by F1-Score\n",
                "metrics_df = metrics_df.sort_values('F1-Score', ascending=False)\n",
                "\n",
                "print(\"\\nüìä Per-Class Metrics (sorted by F1-Score):\")\n",
                "display(metrics_df)\n",
                "\n",
                "# Check classes below threshold\n",
                "threshold = 0.85\n",
                "low_f1_classes = metrics_df[metrics_df['F1-Score'] < threshold]\n",
                "\n",
                "if len(low_f1_classes) > 0:\n",
                "    print(f\"\\n‚ö†Ô∏è  Classes with F1-Score < {threshold}:\")\n",
                "    display(low_f1_classes)\n",
                "else:\n",
                "    print(f\"\\n‚úÖ All classes have F1-Score >= {threshold}!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Confusion Matrix\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "\n",
                "print(f\"üìä Confusion Matrix shape: {cm.shape}\")\n",
                "print(f\"   Diagonal sum (correct predictions): {np.trace(cm):,}\")\n",
                "print(f\"   Off-diagonal sum (misclassifications): {cm.sum() - np.trace(cm):,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7.1 Visualize Confusion Matrix\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create confusion matrix heatmap\n",
                "plt.figure(figsize=(20, 18))\n",
                "\n",
                "# Normalize confusion matrix for better visualization\n",
                "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "\n",
                "# Create heatmap\n",
                "sns.heatmap(cm_normalized, annot=False, fmt='.2f', cmap='Blues', \n",
                "            xticklabels=[label_mapping[i] for i in range(len(cm))],\n",
                "            yticklabels=[label_mapping[i] for i in range(len(cm))],\n",
                "            cbar_kws={'label': 'Normalized Count'})\n",
                "\n",
                "plt.title('Confusion Matrix (Normalized)\\nIoT Attack Detection - 34 Classes', \n",
                "         fontsize=16, fontweight='bold', pad=20)\n",
                "plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
                "plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
                "plt.xticks(rotation=90, ha='right')\n",
                "plt.yticks(rotation=0)\n",
                "plt.tight_layout()\n",
                "\n",
                "# Save figure\n",
                "output_path = '../Output/metrics/confusion_matrix.png'\n",
                "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
                "print(f\"üíæ Confusion matrix saved to: {output_path}\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Training History Visualization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load training history\n",
                "history_path = '../Output/metrics/training_history.json'\n",
                "with open(history_path, 'r') as f:\n",
                "    training_history = json.load(f)\n",
                "\n",
                "# Extract data\n",
                "rounds = training_history['history']['round']\n",
                "accuracy = training_history['history']['accuracy']\n",
                "loss = training_history['history']['loss']\n",
                "\n",
                "# Create plots\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Plot accuracy\n",
                "axes[0].plot(rounds, accuracy, marker='o', linewidth=2.5, markersize=7, color='#2E86AB')\n",
                "axes[0].axhline(y=0.95, color='red', linestyle='--', linewidth=2, label='Target (95%)', alpha=0.7)\n",
                "axes[0].fill_between(rounds, 0, accuracy, alpha=0.2, color='#2E86AB')\n",
                "axes[0].set_title('Global Model Accuracy vs Round', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Round', fontsize=12, fontweight='bold')\n",
                "axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
                "axes[0].set_ylim([0, 1.05])\n",
                "axes[0].grid(True, alpha=0.3, linestyle='--')\n",
                "axes[0].legend(fontsize=11)\n",
                "\n",
                "# Plot loss\n",
                "axes[1].plot(rounds, loss, marker='o', linewidth=2.5, markersize=7, color='#F18F01')\n",
                "axes[1].fill_between(rounds, 0, loss, alpha=0.2, color='#F18F01')\n",
                "axes[1].set_title('Global Model Loss vs Round', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Round', fontsize=12, fontweight='bold')\n",
                "axes[1].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
                "axes[1].grid(True, alpha=0.3, linestyle='--')\n",
                "\n",
                "plt.tight_layout()\n",
                "\n",
                "# Save figure\n",
                "output_path = '../Output/metrics/accuracy_plot.png'\n",
                "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
                "print(f\"üíæ Training curves saved to: {output_path}\")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Per-Class F1-Score Visualization\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create F1-Score bar chart\n",
                "plt.figure(figsize=(18, 8))\n",
                "\n",
                "# Prepare data\n",
                "class_names = [label_mapping[i] for i in range(len(f1))]\n",
                "colors = ['#27AE60' if score >= 0.85 else '#E74C3C' for score in f1]\n",
                "\n",
                "# Create bar chart\n",
                "bars = plt.bar(range(len(f1)), f1, color=colors, edgecolor='black', linewidth=0.5)\n",
                "\n",
                "# Add threshold line\n",
                "plt.axhline(y=0.85, color='red', linestyle='--', linewidth=2, \n",
                "           label='Threshold (0.85)', alpha=0.7)\n",
                "\n",
                "# Customize plot\n",
                "plt.title('F1-Score per Attack Class\\n(Green: ‚â•0.85 | Red: <0.85)', \n",
                "         fontsize=16, fontweight='bold', pad=20)\n",
                "plt.xlabel('Attack Class', fontsize=14, fontweight='bold')\n",
                "plt.ylabel('F1-Score', fontsize=14, fontweight='bold')\n",
                "plt.xticks(range(len(f1)), class_names, rotation=90, ha='right')\n",
                "plt.ylim([0, 1.05])\n",
                "plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
                "plt.legend(fontsize=12)\n",
                "plt.tight_layout()\n",
                "\n",
                "# Save figure\n",
                "output_path = '../Output/metrics/f1_scores_per_class.png'\n",
                "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
                "print(f\"üíæ F1-Score chart saved to: {output_path}\")\n",
                "\n",
                "plt.show()\n",
                "\n",
                "# Print summary\n",
                "num_above_threshold = np.sum(f1 >= 0.85)\n",
                "num_below_threshold = np.sum(f1 < 0.85)\n",
                "\n",
                "print(f\"\\nüìä F1-Score Summary:\")\n",
                "print(f\"   Classes with F1 ‚â• 0.85: {num_above_threshold}/{len(f1)} ({num_above_threshold/len(f1)*100:.1f}%)\")\n",
                "print(f\"   Classes with F1 < 0.85: {num_below_threshold}/{len(f1)} ({num_below_threshold/len(f1)*100:.1f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Export Comprehensive Metrics Report\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare comprehensive metrics report\n",
                "metrics_report = {\n",
                "    'overall_metrics': {\n",
                "        'accuracy': float(overall_accuracy),\n",
                "        'precision_macro': float(precision_macro),\n",
                "        'recall_macro': float(recall_macro),\n",
                "        'f1_macro': float(f1_macro),\n",
                "        'precision_weighted': float(precision_weighted),\n",
                "        'recall_weighted': float(recall_weighted),\n",
                "        'f1_weighted': float(f1_weighted)\n",
                "    },\n",
                "    'per_class_metrics': {},\n",
                "    'summary': {\n",
                "        'total_test_samples': int(len(y_test)),\n",
                "        'num_classes': int(len(label_mapping)),\n",
                "        'classes_above_f1_threshold': int(num_above_threshold),\n",
                "        'classes_below_f1_threshold': int(num_below_threshold),\n",
                "        'target_accuracy_met': bool(overall_accuracy >= 0.95),\n",
                "        'all_classes_above_threshold': bool(num_below_threshold == 0)\n",
                "    },\n",
                "    'confusion_matrix': cm.tolist()\n",
                "}\n",
                "\n",
                "# Add per-class metrics\n",
                "for i in range(len(precision)):\n",
                "    class_name = label_mapping[i]\n",
                "    metrics_report['per_class_metrics'][class_name] = {\n",
                "        'class_id': int(i),\n",
                "        'precision': float(precision[i]),\n",
                "        'recall': float(recall[i]),\n",
                "        'f1_score': float(f1[i]),\n",
                "        'support': int(support[i])\n",
                "    }\n",
                "\n",
                "# Save to JSON\n",
                "report_path = '../Output/metrics/metrics_report.json'\n",
                "with open(report_path, 'w') as f:\n",
                "    json.dump(metrics_report, f, indent=2)\n",
                "\n",
                "print(f\"üíæ Comprehensive metrics report saved to: {report_path}\")\n",
                "print(f\"\\n‚úÖ Report includes:\")\n",
                "print(f\"   - Overall metrics (accuracy, precision, recall, F1)\")\n",
                "print(f\"   - Per-class metrics for all {len(label_mapping)} classes\")\n",
                "print(f\"   - Confusion matrix\")\n",
                "print(f\"   - Summary statistics\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Generate Classification Report\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate sklearn classification report\n",
                "target_names = [label_mapping[i] for i in range(len(label_mapping))]\n",
                "report = classification_report(y_test, y_pred, target_names=target_names, zero_division=0)\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"CLASSIFICATION REPORT\")\n",
                "print(\"=\"*80)\n",
                "print(report)\n",
                "\n",
                "# Save to text file\n",
                "report_txt_path = '../Output/metrics/classification_report.txt'\n",
                "with open(report_txt_path, 'w') as f:\n",
                "    f.write(\"CLASSIFICATION REPORT\\n\")\n",
                "    f.write(\"=\"*80 + \"\\n\")\n",
                "    f.write(report)\n",
                "\n",
                "print(f\"\\nüíæ Classification report saved to: {report_txt_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Final Summary\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"MODEL EVALUATION SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(f\"\\nüìä Performance Metrics:\")\n",
                "print(f\"   Overall Accuracy: {overall_accuracy*100:.2f}%\")\n",
                "print(f\"   Macro F1-Score: {f1_macro:.4f}\")\n",
                "print(f\"   Weighted F1-Score: {f1_weighted:.4f}\")\n",
                "\n",
                "print(f\"\\nüéØ Target Achievement:\")\n",
                "if overall_accuracy >= 0.95:\n",
                "    print(f\"   ‚úÖ Accuracy target (>95%): ACHIEVED\")\n",
                "else:\n",
                "    print(f\"   ‚ùå Accuracy target (>95%): NOT ACHIEVED (Gap: {(0.95-overall_accuracy)*100:.2f}%)\")\n",
                "\n",
                "if num_below_threshold == 0:\n",
                "    print(f\"   ‚úÖ F1-Score target (>0.85 for all classes): ACHIEVED\")\n",
                "else:\n",
                "    print(f\"   ‚ö†Ô∏è  F1-Score target: {num_below_threshold} classes below 0.85\")\n",
                "\n",
                "print(f\"\\nüìÅ Generated Files:\")\n",
                "output_files = [\n",
                "    '../Output/metrics/confusion_matrix.png',\n",
                "    '../Output/metrics/accuracy_plot.png',\n",
                "    '../Output/metrics/f1_scores_per_class.png',\n",
                "    '../Output/metrics/metrics_report.json',\n",
                "    '../Output/metrics/classification_report.txt'\n",
                "]\n",
                "\n",
                "for file_path in output_files:\n",
                "    if os.path.exists(file_path):\n",
                "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
                "        print(f\"   ‚úì {os.path.basename(file_path)} ({file_size:.2f} KB)\")\n",
                "\n",
                "print(f\"\\nüíæ All Deliverables for Web App:\")\n",
                "deliverables = [\n",
                "    '../Output/models/global_model.h5',\n",
                "    '../Output/models/scaler.pkl',\n",
                "    '../Output/models/label_encoder.pkl',\n",
                "    '../Output/models/labels.json'\n",
                "]\n",
                "\n",
                "for file_path in deliverables:\n",
                "    if os.path.exists(file_path):\n",
                "        file_size = os.path.getsize(file_path) / 1024  # KB\n",
                "        if file_path.endswith('.h5'):\n",
                "            file_size = file_size / 1024  # MB for model\n",
                "            print(f\"   ‚úì {os.path.basename(file_path)} ({file_size:.2f} MB)\")\n",
                "        else:\n",
                "            print(f\"   ‚úì {os.path.basename(file_path)} ({file_size:.2f} KB)\")\n",
                "\n",
                "print(f\"\\n‚úÖ MODEL EVALUATION COMPLETE!\")\n",
                "print(f\"\\nüìù Next steps:\")\n",
                "print(f\"   1. Review all visualizations and metrics\")\n",
                "print(f\"   2. Include plots in thesis report\")\n",
                "print(f\"   3. Use deliverables for Web App integration\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
