# T√ÄI LI·ªÜU GI·∫¢I TH√çCH CHI TI·∫æT: NOTEBOOK TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU
**File g·ªëc:** `1_Data_Preprocessing.ipynb`  
**T√°c gi·∫£ t√†i li·ªáu:** AI Assistant  
**Ng√†y t·∫°o:** 30/12/2025

---

## M·ª§C L·ª§C
1. [T·ªïng quan v·ªÅ Notebook](#1-t·ªïng-quan-v·ªÅ-notebook)
2. [Ph·∫ßn 1: Setup v√† Import th∆∞ vi·ªán](#2-ph·∫ßn-1-setup-v√†-import-th∆∞-vi·ªán)
3. [Ph·∫ßn 2: T·∫£i c·∫•u h√¨nh](#3-ph·∫ßn-2-t·∫£i-c·∫•u-h√¨nh)
4. [Ph·∫ßn 3: T·∫£i v√† Ti·ªÅn x·ª≠ l√Ω Dataset](#4-ph·∫ßn-3-t·∫£i-v√†-ti·ªÅn-x·ª≠-l√Ω-dataset)
5. [Ph·∫ßn 4: L·ªçc ƒë·∫∑c tr∆∞ng b·∫±ng GSA](#5-ph·∫ßn-4-l·ªçc-ƒë·∫∑c-tr∆∞ng-b·∫±ng-gsa)
6. [Ph·∫ßn 5: M√£ h√≥a nh√£n](#6-ph·∫ßn-5-m√£-h√≥a-nh√£n)
7. [Ph·∫ßn 6: Chu·∫©n h√≥a Features](#7-ph·∫ßn-6-chu·∫©n-h√≥a-features)
8. [Ph·∫ßn 7: Ph√¢n chia d·ªØ li·ªáu cho FL](#8-ph·∫ßn-7-ph√¢n-chia-d·ªØ-li·ªáu-cho-federated-learning)
9. [Ph·∫ßn 8-10: L∆∞u tr·ªØ v√† Verification](#9-ph·∫ßn-8-10-l∆∞u-tr·ªØ-v√†-verification)

---

## 1. T·ªîNG QUAN V·ªÄ NOTEBOOK

### 1.1 M·ª•c ti√™u ch√≠nh
Notebook n√†y l√† **b∆∞·ªõc ƒë·∫ßu ti√™n** trong pipeline hu·∫•n luy·ªán m√¥ h√¨nh Federated Learning ƒë·ªÉ ph√°t hi·ªán t·∫•n c√¥ng m·∫°ng IoT. N√≥ th·ª±c hi·ªán:

1. **T·∫£i d·ªØ li·ªáu kh·ªïng l·ªì** (~12GB, 169 file CSV) t·ª´ b·ªô dataset CICIoT2023
2. **L√†m s·∫°ch d·ªØ li·ªáu** (lo·∫°i b·ªè gi√° tr·ªã null, duplicate)
3. **L·ªçc ƒë·∫∑c tr∆∞ng** b·∫±ng thu·∫≠t to√°n GSA (t·ª´ 46 ‚Üí 22 features)
4. **M√£ h√≥a nh√£n** (chuy·ªÉn 34 t√™n t·∫•n c√¥ng th√†nh s·ªë 0-33)
5. **Chu·∫©n h√≥a** t·∫•t c·∫£ features v·ªÅ kho·∫£ng [0, 1]
6. **Ph√¢n chia d·ªØ li·ªáu** cho 5 m√°y kh√°ch (clients) theo chi·∫øn l∆∞·ª£c Non-IID

### 1.2 ƒê·∫ßu ra (Outputs) quan tr·ªçng
Sau khi ch·∫°y xong, b·∫°n s·∫Ω c√≥:
- `client_0_data.npz` ƒë·∫øn `client_4_data.npz`: D·ªØ li·ªáu cho 5 clients
- `test_data.npz`: D·ªØ li·ªáu test chung
- `scaler.pkl`: B·ªô chu·∫©n h√≥a (MinMaxScaler)
- `label_encoder.pkl` & `labels.json`: B·∫£ng m√£ h√≥a nh√£n

---

## 2. PH·∫¶N 1: SETUP V√Ä IMPORT TH∆Ø VI·ªÜN

### 2.1 Code g·ªëc
```python
# Standard libraries
import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import yaml
from pathlib import Path

# Import our utility modules
from utils import data_utils
from utils import fl_utils_pytorch
from utils.includes import X_columns, y_column, dict_34_classes

# Set display options
pd.set_option('display.max_columns', 50)
pd.set_option('display.max_rows', 100)

# Set random seed for reproducibility
np.random.seed(42)
```

### 2.2 Gi·∫£i th√≠ch chi ti·∫øt

#### A. Th∆∞ vi·ªán chu·∫©n (Standard Libraries)
- **`os`, `sys`**: Qu·∫£n l√Ω ƒë∆∞·ªùng d·∫´n file, th∆∞ m·ª•c
- **`numpy`**: T√≠nh to√°n s·ªë h·ªçc, x·ª≠ l√Ω ma tr·∫≠n
- **`pandas`**: X·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng b·∫£ng (DataFrame)
- **`matplotlib`, `seaborn`**: V·∫Ω bi·ªÉu ƒë·ªì, tr·ª±c quan h√≥a
- **`yaml`**: ƒê·ªçc file c·∫•u h√¨nh `.yaml`
- **`pathlib.Path`**: X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n hi·ªán ƒë·∫°i h∆°n `os.path`

#### B. Module t·ª± vi·∫øt (Custom Utilities)
- **`data_utils`**: Ch·ª©a c√°c h√†m:
  - `clean_data()`: Lo·∫°i b·ªè null, duplicate
  - `encode_labels()`: Chuy·ªÉn t√™n t·∫•n c√¥ng ‚Üí s·ªë
  - `normalize_features()`: Chu·∫©n h√≥a v·ªÅ [0,1]
  - `partition_data_noniid()`: Chia d·ªØ li·ªáu Non-IID
  
- **`fl_utils_pytorch`**: C√°c h√†m h·ªó tr·ª£ Federated Learning (s·∫Ω d√πng ·ªü Notebook 2)

- **`utils.includes`**: ƒê·ªãnh nghƒ©a:
  - `X_columns`: Danh s√°ch 46 t√™n c·ªôt ƒë·∫∑c tr∆∞ng
  - `y_column`: T√™n c·ªôt nh√£n (`'label'`)
  - `dict_34_classes`: Dictionary √°nh x·∫° 34 lo·∫°i t·∫•n c√¥ng

#### C. C·∫•u h√¨nh hi·ªÉn th·ªã
```python
pd.set_option('display.max_columns', 50)  # Hi·ªÉn th·ªã t·ªëi ƒëa 50 c·ªôt
pd.set_option('display.max_rows', 100)    # Hi·ªÉn th·ªã t·ªëi ƒëa 100 d√≤ng
```
**T·∫°i sao c·∫ßn?** Dataset c√≥ 47 c·ªôt, n·∫øu kh√¥ng set th√¨ Pandas s·∫Ω ·∫©n b·ªõt c·ªôt khi in ra.

#### D. Random Seed
```python
np.random.seed(42)
```
**T·∫°i sao c·∫ßn?** ƒê·∫£m b·∫£o k·∫øt qu·∫£ **c√≥ th·ªÉ t√°i t·∫°o** (reproducible). M·ªói l·∫ßn ch·∫°y code s·∫Ω cho k·∫øt qu·∫£ gi·ªëng nhau.

### 2.3 Output khi ch·∫°y
```
‚úÖ All imports successful!
   Number of features: 46
   Label column: label
   Number of attack classes: 34
```

---

## 3. PH·∫¶N 2: T·∫¢I C·∫§U H√åNH

### 3.1 Code g·ªëc
```python
# Load training configuration
config_path = 'configs/training_config.yaml'

with open(config_path, 'r') as f:
    config = yaml.safe_load(f)

print("üìÑ Configuration loaded:")
print(f"   Number of clients: {config['num_clients']}")
print(f"   Test split ratio: {config['data']['test_split_ratio']}")
print(f"   Chunk size: {config['data']['chunk_size']}")
print(f"   Partition strategy: {config['data']['partition_strategy']}")
print(f"   Use sample data: {config['experimental']['use_sample_data']}")
```

### 3.2 Gi·∫£i th√≠ch chi ti·∫øt

#### A. File `training_config.yaml` ch·ª©a g√¨?
ƒê√¢y l√† file c·∫•u h√¨nh t·∫≠p trung, ch·ª©a t·∫•t c·∫£ tham s·ªë quan tr·ªçng:

```yaml
num_clients: 5                    # S·ªë l∆∞·ª£ng m√°y kh√°ch (clients)
random_seed: 42                   # Seed cho random

data:
  test_split_ratio: 0.2           # 20% d·ªØ li·ªáu d√πng ƒë·ªÉ test
  chunk_size: 50000               # ƒê·ªçc 50,000 d√≤ng m·ªôt l√∫c (ti·∫øt ki·ªám RAM)
  partition_strategy: 'non_iid'   # Chi·∫øn l∆∞·ª£c chia d·ªØ li·ªáu

experimental:
  use_sample_data: false          # C√≥ d√πng d·ªØ li·ªáu m·∫´u nh·ªè kh√¥ng?
  sample_fraction: 0.05           # N·∫øu c√≥, l·∫•y 5% d·ªØ li·ªáu
```

#### B. T·∫°i sao d√πng file YAML thay v√¨ hard-code?
**∆Øu ƒëi·ªÉm:**
1. **D·ªÖ thay ƒë·ªïi**: Kh√¥ng c·∫ßn s·ª≠a code, ch·ªâ c·∫ßn s·ª≠a file YAML
2. **T√°i s·ª≠ d·ª•ng**: C√≥ th·ªÉ t·∫°o nhi·ªÅu file config kh√°c nhau cho c√°c th√≠ nghi·ªám
3. **D·ªÖ ƒë·ªçc**: C√∫ ph√°p YAML r·∫•t d·ªÖ hi·ªÉu

#### C. C√°c tham s·ªë quan tr·ªçng

**`num_clients: 5`**
- Chia d·ªØ li·ªáu cho **5 m√°y kh√°ch** (m√¥ ph·ªèng 5 thi·∫øt b·ªã IoT kh√°c nhau)
- Trong th·ª±c t·∫ø c√≥ th·ªÉ l√† 10, 20, 100 clients

**`test_split_ratio: 0.2`**
- 20% d·ªØ li·ªáu d√πng ƒë·ªÉ **test** (ƒë√°nh gi√° cu·ªëi c√πng)
- 80% c√≤n l·∫°i d√πng ƒë·ªÉ **train** (chia cho 5 clients)

**`chunk_size: 50000`**
- Khi ƒë·ªçc file CSV l·ªõn, ch·ªâ ƒë·ªçc 50,000 d√≤ng m·ªôt l√∫c v√†o RAM
- **T·∫°i sao?** Tr√°nh tr√†n RAM khi file qu√° l·ªõn (12GB)

**`partition_strategy: 'non_iid'`**
- **IID** (Independent and Identically Distributed): M·ªói client c√≥ d·ªØ li·ªáu gi·ªëng nhau
- **Non-IID**: M·ªói client c√≥ d·ªØ li·ªáu **kh√°c nhau** (th·ª±c t·∫ø h∆°n)
- V√≠ d·ª•: Client 1 ch·ªß y·∫øu th·∫•y t·∫•n c√¥ng DDoS, Client 2 ch·ªß y·∫øu th·∫•y Malware

### 3.3 Output khi ch·∫°y
```
üìÑ Configuration loaded:
   Number of clients: 5
   Test split ratio: 0.2
   Chunk size: 50000
   Partition strategy: non_iid
   Use sample data: False
```

---

## 4. PH·∫¶N 3: T·∫¢I V√Ä TI·ªÄN X·ª¨ L√ù DATASET

### 4.1 T·ªïng quan chi·∫øn l∆∞·ª£c
ƒê√¢y l√† ph·∫ßn **QUAN TR·ªåNG NH·∫§T** v√† c≈©ng **PH·ª®C T·∫†P NH·∫§T** c·ªßa Notebook. Code th·ª±c hi·ªán:

1. **Ki·ªÉm tra Cache**: N·∫øu ƒë√£ x·ª≠ l√Ω r·ªìi ‚Üí t·∫£i tr·ª±c ti·∫øp
2. **N·∫øu ch∆∞a c√≥ Cache**: Ch·∫°y pipeline ƒë·∫ßy ƒë·ªß:
   - T·∫£i 169 file CSV
   - Merge th√†nh 1 DataFrame
   - **SHUFFLE** (c·ª±c k·ª≥ quan tr·ªçng!)
   - Chia Train/Test
   - L∆∞u v√†o Cache

### 4.2 Code ph·∫ßn CACHE HIT (ƒê√£ x·ª≠ l√Ω r·ªìi)

```python
if os.path.exists(train_file) and os.path.exists(test_file):
    print("CACHE HIT: Loading preprocessed datasets from disk")
    
    df_train = pd.read_csv(train_file, low_memory=False)
    df_test = pd.read_csv(test_file, low_memory=False)
    
    print(f"‚úÖ Loaded from cache successfully!")
    print(f"   Train shape: {df_train.shape}")
    print(f"   Test shape: {df_test.shape}")
```

#### Gi·∫£i th√≠ch:
- **`train_file`**: `../Output/preprocessed/train_dataset.csv`
- **`test_file`**: `../Output/preprocessed/test_dataset.csv`
- **`low_memory=False`**: ƒê·ªçc to√†n b·ªô file v√†o RAM (nhanh h∆°n nh∆∞ng t·ªën RAM)

**K·∫øt qu·∫£:**
```
Train shape: (2487431, 47)  # 2.4 tri·ªáu d√≤ng, 47 c·ªôt
Test shape: (130917, 47)    # 130 ngh√¨n d√≤ng, 47 c·ªôt
```

### 4.3 Code ph·∫ßn CACHE MISS (Ch∆∞a x·ª≠ l√Ω)

#### B∆Ø·ªöC 1: T·∫£i t·∫•t c·∫£ file CSV

```python
csv_pattern = os.path.join(data_dir, '*.csv')
csv_files = sorted(glob.glob(csv_pattern))

dataframes = []
total_rows = 0

for i, file_path in enumerate(csv_files):
    try:
        df_file = pd.read_csv(file_path, low_memory=False)
        rows = len(df_file)
        total_rows += rows
        dataframes.append(df_file)
        
        if (i + 1) % 20 == 0:  # In progress m·ªói 20 file
            print(f"[{i+1}/{len(csv_files)}] Loaded {os.path.basename(file_path)}")
    except Exception as e:
        print(f"‚ö†Ô∏è Error loading {file_path}: {e}")
        continue
```

**Gi·∫£i th√≠ch t·ª´ng d√≤ng:**

1. **`glob.glob(csv_pattern)`**: T√¨m t·∫•t c·∫£ file `.csv` trong th∆∞ m·ª•c `../DataTests`
2. **`sorted(...)`**: S·∫Øp x·∫øp theo t√™n file (ƒë·∫£m b·∫£o th·ª© t·ª± nh·∫•t qu√°n)
3. **`dataframes.append(df_file)`**: Th√™m DataFrame v√†o list
4. **`if (i + 1) % 20 == 0`**: Ch·ªâ in th√¥ng b√°o m·ªói 20 file (tr√°nh spam console)
5. **`try...except`**: N·∫øu file b·ªã l·ªói, b·ªè qua v√† ti·∫øp t·ª•c

**T·∫°i sao kh√¥ng d√πng `pd.concat()` ngay?**
- V√¨ `concat()` t·ªën RAM. T·ªët h∆°n l√† load h·∫øt v√†o list tr∆∞·ªõc, r·ªìi concat 1 l·∫ßn.

#### B∆Ø·ªöC 2: Merge t·∫•t c·∫£ DataFrame

```python
df_merged = pd.concat(dataframes, ignore_index=True)
print(f"‚úÖ Merged DataFrame created!")
print(f"   Shape: {df_merged.shape}")
print(f"   Memory usage: {df_merged.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

del dataframes  # Gi·∫£i ph√≥ng RAM
```

**Gi·∫£i th√≠ch:**
- **`pd.concat(dataframes, ignore_index=True)`**: Gh√©p t·∫•t c·∫£ DataFrame th√†nh 1
- **`ignore_index=True`**: T·∫°o l·∫°i index t·ª´ 0 (kh√¥ng gi·ªØ index c≈©)
- **`del dataframes`**: X√≥a bi·∫øn ƒë·ªÉ gi·∫£i ph√≥ng RAM

#### B∆Ø·ªöC 3: SHUFFLE (X√ÅO TR·ªòN) - C·ª∞C K·ª≤ QUAN TR·ªåNG!

```python
df_shuffled = df_merged.sample(frac=1.0, random_state=random_seed).reset_index(drop=True)
```

**Gi·∫£i th√≠ch:**
- **`sample(frac=1.0)`**: L·∫•y m·∫´u 100% d·ªØ li·ªáu (t·ª©c l√† l·∫•y h·∫øt, nh∆∞ng theo th·ª© t·ª± ng·∫´u nhi√™n)
- **`random_state=42`**: ƒê·∫£m b·∫£o shuffle gi·ªëng nhau m·ªói l·∫ßn ch·∫°y
- **`reset_index(drop=True)`**: T·∫°o l·∫°i index t·ª´ 0

---

## 5. PH·∫¶N 4: L·ªåC ƒê·∫∂C TR∆ØNG B·∫∞NG GSA (FEATURE SELECTION)

### 5.1 T·∫°i sao c·∫ßn l·ªçc ƒë·∫∑c tr∆∞ng?

Dataset ban ƒë·∫ßu c√≥ **46 ƒë·∫∑c tr∆∞ng** (features). Nh∆∞ng kh√¥ng ph·∫£i t·∫•t c·∫£ ƒë·ªÅu h·ªØu √≠ch:
- M·ªôt s·ªë features c√≥ th·ªÉ l√† **nhi·ªÖu** (noise) - kh√¥ng li√™n quan ƒë·∫øn vi·ªác ph√¢n lo·∫°i
- M·ªôt s·ªë features c√≥ th·ªÉ **d∆∞ th·ª´a** (redundant) - ch·ª©a th√¥ng tin tr√πng l·∫∑p
- Qu√° nhi·ªÅu features ‚Üí **Overfitting** (m√¥ h√¨nh h·ªçc thu·ªôc l√≤ng thay v√¨ h·ªçc quy lu·∫≠t)

**Gi·∫£i ph√°p:** S·ª≠ d·ª•ng thu·∫≠t to√°n **GSA (Gravitational Search Algorithm)** ƒë·ªÉ t·ª± ƒë·ªông ch·ªçn ra **20 features t·ªët nh·∫•t**.

### 5.2 GSA l√† g√¨?

GSA (Gravitational Search Algorithm) l√† thu·∫≠t to√°n t·ªëi ∆∞u h√≥a l·∫•y c·∫£m h·ª©ng t·ª´ **ƒë·ªãnh lu·∫≠t v·∫°n v·∫≠t h·∫•p d·∫´n** c·ªßa Newton.

**√ù t∆∞·ªüng c∆° b·∫£n:**
1. M·ªói "h·∫°t" (particle) ƒë·∫°i di·ªán cho m·ªôt **b·ªô features** (v√≠ d·ª•: ch·ªçn 20 trong 46 features)
2. "Kh·ªëi l∆∞·ª£ng" c·ªßa h·∫°t = **ƒê·ªô ch√≠nh x√°c** khi d√πng b·ªô features ƒë√≥ ƒë·ªÉ ph√¢n lo·∫°i
3. C√°c h·∫°t c√≥ kh·ªëi l∆∞·ª£ng l·ªõn (ƒë·ªô ch√≠nh x√°c cao) s·∫Ω **h√∫t** c√°c h·∫°t kh√°c v·ªÅ ph√≠a m√¨nh
4. Sau nhi·ªÅu v√≤ng l·∫∑p, c√°c h·∫°t s·∫Ω **h·ªôi t·ª•** v·ªÅ b·ªô features t·ªët nh·∫•t

### 5.3 Code chi ti·∫øt

```python
# C·∫•u h√¨nh GSA
gsa_config = {
    'enabled': True,
    'target_features': 20,      # Ch·ªçn 20 features
    'population_size': 15,      # 15 "h·∫°t" trong qu·∫ßn th·ªÉ
    'max_iterations': 30,       # Ch·∫°y t·ªëi ƒëa 30 v√≤ng l·∫∑p
    'sample_fraction': 0.05     # D√πng 5% d·ªØ li·ªáu ƒë·ªÉ tƒÉng t·ªëc
}

# Ch·∫°y GSA
from utils.gsa_algorithm import GSA

gsa = GSA(
    n_features=len(X_columns),
    target_features=20,
    population_size=15,
    max_iterations=30
)

# L·∫•y m·∫´u 5% d·ªØ li·ªáu
sample_size = int(len(df_train) * 0.05)
df_sample = df_train.sample(n=sample_size, random_state=42)

X_sample = df_sample[X_columns].values
y_sample = df_sample[y_column].values

# Ch·∫°y t·ªëi ∆∞u h√≥a
selected_indices = gsa.optimize(X_sample, y_sample)
selected_features = [X_columns[i] for i in selected_indices]
```

**Gi·∫£i th√≠ch t·ª´ng d√≤ng:**

1. **`target_features=20`**: M·ª•c ti√™u l√† ch·ªçn 20 features t·ªët nh·∫•t
2. **`population_size=15`**: C√≥ 15 "h·∫°t" trong qu·∫ßn th·ªÉ (m·ªói h·∫°t l√† 1 b·ªô 20 features)
3. **`max_iterations=30`**: Ch·∫°y t·ªëi ƒëa 30 v√≤ng l·∫∑p ƒë·ªÉ t√¨m b·ªô features t·ªët nh·∫•t
4. **`sample_fraction=0.05`**: Ch·ªâ d√πng 5% d·ªØ li·ªáu ƒë·ªÉ tƒÉng t·ªëc (v√¨ GSA ch·∫°y r·∫•t l√¢u)

### 5.4 K·∫øt qu·∫£

Sau khi ch·∫°y GSA, ta ƒë∆∞·ª£c:
- **22 features ƒë∆∞·ª£c ch·ªçn** (thay v√¨ 20 nh∆∞ m·ª•c ti√™u - c√≥ th·ªÉ do thu·∫≠t to√°n ƒëi·ªÅu ch·ªânh)
- **Fitness/Accuracy: 0.9568** (95.68% ƒë·ªô ch√≠nh x√°c tr√™n t·∫≠p m·∫´u)
- **Gi·∫£m 52.2% k√≠ch th∆∞·ªõc** (t·ª´ 46 ‚Üí 22 features)

**L·ª£i √≠ch:**
- M√¥ h√¨nh ch·∫°y **nhanh h∆°n** (√≠t features h∆°n)
- **Gi·∫£m overfitting** (lo·∫°i b·ªè nhi·ªÖu)
- **TƒÉng ƒë·ªô ch√≠nh x√°c** (ch·ªâ gi·ªØ l·∫°i features quan tr·ªçng)

---

## 6. PH·∫¶N 5: M√É H√ìA NH√ÉN (ENCODE LABELS)

### 6.1 T·∫°i sao c·∫ßn m√£ h√≥a nh√£n?

M√°y t√≠nh **kh√¥ng hi·ªÉu ch·ªØ**, ch·ªâ hi·ªÉu **s·ªë**. Trong dataset, c·ªôt `label` ch·ª©a t√™n c√°c lo·∫°i t·∫•n c√¥ng d∆∞·ªõi d·∫°ng text:
- `"DDoS-ICMP_Flood"`
- `"BenignTraffic"`
- `"Mirai-greeth_flood"`
- ...

Ta c·∫ßn chuy·ªÉn ch√∫ng th√†nh **s·ªë** ƒë·ªÉ m√°y t√≠nh c√≥ th·ªÉ x·ª≠ l√Ω.

### 6.2 Code chi ti·∫øt

```python
from sklearn.preprocessing import LabelEncoder
import json

# T·∫°o LabelEncoder
label_encoder = LabelEncoder()

# M√£ h√≥a nh√£n
df_train[y_column] = label_encoder.fit_transform(df_train[y_column])

# L∆∞u label encoder ƒë·ªÉ d√πng sau n√†y
import pickle
with open('../Output/models/label_encoder.pkl', 'wb') as f:
    pickle.dump(label_encoder, f)

# L∆∞u mapping (√°nh x·∫°) t·ª´ s·ªë ‚Üí t√™n
label_mapping = {i: label for i, label in enumerate(label_encoder.classes_)}
with open('../Output/models/labels.json', 'w') as f:
    json.dump(label_mapping, f, indent=2)
```

**Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc:**

1. **`LabelEncoder()`**: T·∫°o b·ªô m√£ h√≥a nh√£n
2. **`fit_transform()`**: H·ªçc v√† chuy·ªÉn ƒë·ªïi nh√£n th√†nh s·ªë
   - V√≠ d·ª•: `"DDoS-ICMP_Flood"` ‚Üí `6`
   - `"BenignTraffic"` ‚Üí `1`
3. **L∆∞u `label_encoder.pkl`**: ƒê·ªÉ sau n√†y c√≥ th·ªÉ decode ng∆∞·ª£c l·∫°i
4. **L∆∞u `labels.json`**: File JSON d·ªÖ ƒë·ªçc, ch·ª©a mapping:
   ```json
   {
     "0": "Backdoor_Malware",
     "1": "BenignTraffic",
     "6": "DDoS-ICMP_Flood",
     ...
   }
   ```

### 6.3 K·∫øt qu·∫£

Sau khi m√£ h√≥a:
- **34 lo·∫°i t·∫•n c√¥ng** ‚Üí **S·ªë t·ª´ 0 ƒë·∫øn 33**
- C·ªôt `label` gi·ªù ch·ª©a s·ªë thay v√¨ text
- V√≠ d·ª•: `"DDoS-ICMP_Flood"` ‚Üí `6`

---

## 7. PH·∫¶N 6: CHU·∫®N H√ìA FEATURES (NORMALIZE)

### 7.1 T·∫°i sao c·∫ßn chu·∫©n h√≥a?

C√°c features c√≥ **ƒë∆°n v·ªã kh√°c nhau**:
- `flow_duration`: C√≥ th·ªÉ t·ª´ 0 ƒë·∫øn v√†i tri·ªáu (microseconds)
- `fin_flag_number`: Ch·ªâ c√≥ gi√° tr·ªã 0 ho·∫∑c 1
- `Rate`: C√≥ th·ªÉ t·ª´ 0 ƒë·∫øn v√†i ngh√¨n

N·∫øu kh√¥ng chu·∫©n h√≥a:
- Features c√≥ gi√° tr·ªã l·ªõn s·∫Ω **chi ph·ªëi** qu√° tr√¨nh h·ªçc
- M√¥ h√¨nh Deep Learning s·∫Ω **h·ªçc ch·∫≠m** ho·∫∑c **kh√¥ng h·ªôi t·ª•**

**Gi·∫£i ph√°p:** D√πng **MinMaxScaler** ƒë·ªÉ ƒë∆∞a t·∫•t c·∫£ v·ªÅ kho·∫£ng `[0, 1]`.

### 7.2 Code chi ti·∫øt

```python
from sklearn.preprocessing import MinMaxScaler

# T·∫°o scaler
scaler = MinMaxScaler()

# Chu·∫©n h√≥a features
df_train[X_columns] = scaler.fit_transform(df_train[X_columns])

# L∆∞u scaler ƒë·ªÉ d√πng cho d·ªØ li·ªáu m·ªõi
import pickle
with open('../Output/models/scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
```

**Gi·∫£i th√≠ch:**

1. **`MinMaxScaler()`**: T·∫°o b·ªô chu·∫©n h√≥a
2. **`fit_transform()`**: H·ªçc min/max c·ªßa m·ªói feature v√† chu·∫©n h√≥a
   - C√¥ng th·ª©c: `X_scaled = (X - X_min) / (X_max - X_min)`
   - K·∫øt qu·∫£: T·∫•t c·∫£ gi√° tr·ªã n·∫±m trong `[0, 1]`
3. **L∆∞u `scaler.pkl`**: **C·ª∞C K·ª≤ QUAN TR·ªåNG!**
   - Khi c√≥ d·ªØ li·ªáu m·ªõi (g√≥i tin m·∫°ng m·ªõi), b·∫°n **PH·∫¢I** d√πng scaler n√†y ƒë·ªÉ chu·∫©n h√≥a
   - N·∫øu kh√¥ng, m√¥ h√¨nh s·∫Ω d·ª± ƒëo√°n sai!

### 7.3 V√≠ d·ª• minh h·ªça

**Tr∆∞·ªõc khi chu·∫©n h√≥a:**
```
flow_duration: 4.888106
Rate: 0.409156
fin_flag_number: 0.0
```

**Sau khi chu·∫©n h√≥a:**
```
flow_duration: 0.0000488  (ƒë√£ chia cho max)
Rate: 0.0000041
fin_flag_number: 0.0
```

---

## 8. PH·∫¶N 7: PH√ÇN CHIA D·ªÆ LI·ªÜU CHO FEDERATED LEARNING

### 8.1 Chi·∫øn l∆∞·ª£c Non-IID

**IID (Independent and Identically Distributed):**
- M·ªói Client c√≥ d·ªØ li·ªáu **gi·ªëng h·ªát nhau**
- V√≠ d·ª•: Client 1, 2, 3, 4, 5 ƒë·ªÅu c√≥ 20% m·ªói lo·∫°i t·∫•n c√¥ng

**Non-IID:**
- M·ªói Client c√≥ d·ªØ li·ªáu **kh√°c nhau**
- V√≠ d·ª•:
  - Client 1: 80% DDoS, 20% Benign
  - Client 2: 90% Mirai, 10% DDoS
  - Client 3: 70% Benign, 30% Web attacks

**T·∫°i sao d√πng Non-IID?**
- Trong th·ª±c t·∫ø, m·ªói thi·∫øt b·ªã IoT ch·ªâ th·∫•y **m·ªôt ph·∫ßn** l∆∞u l∆∞·ª£ng m·∫°ng
- M√¥ ph·ªèng ƒë√∫ng m√¥i tr∆∞·ªùng th·ª±c t·∫ø c·ªßa Federated Learning

### 8.2 Code chi ti·∫øt

```python
def partition_data_noniid(df_train, num_clients=5, label_col='label', 
                          test_split=0.2, random_seed=42):
    # Chia train/test
    train_data, test_data = train_test_split(
        df_train, test_size=test_split, 
        stratify=df_train[label_col],  # ƒê·∫£m b·∫£o t·ª∑ l·ªá nh√£n ƒë·ªìng ƒë·ªÅu
        random_state=random_seed
    )
    
    # S·∫Øp x·∫øp theo nh√£n
    train_data = train_data.sort_values(by=label_col)
    
    # Chia th√†nh c√°c "shard" (m·∫£nh)
    num_shards = num_clients * 2
    shard_size = len(train_data) // num_shards
    
    # G√°n shard cho t·ª´ng client (m·ªói client nh·∫≠n 2 shards ng·∫´u nhi√™n)
    client_data = {}
    for i in range(num_clients):
        # Ch·ªçn 2 shards ng·∫´u nhi√™n
        shard_indices = np.random.choice(num_shards, 2, replace=False)
        
        # L·∫•y d·ªØ li·ªáu t·ª´ 2 shards
        client_df = pd.concat([
            train_data[shard_indices[0]*shard_size:(shard_indices[0]+1)*shard_size],
            train_data[shard_indices[1]*shard_size:(shard_indices[1]+1)*shard_size]
        ])
        
        client_data[f'client_{i}'] = {
            'X': client_df[X_columns].values,
            'y': client_df[label_col].values
        }
    
    # Test set chung
    client_data['test'] = {
        'X': test_data[X_columns].values,
        'y': test_data[label_col].values
    }
    
    return client_data
```

**Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc:**

1. **Chia Train/Test:**
   - 80% train, 20% test
   - `stratify`: ƒê·∫£m b·∫£o t·ª∑ l·ªá c√°c nh√£n gi·ªëng nhau ·ªü train v√† test

2. **S·∫Øp x·∫øp theo nh√£n:**
   - ƒê·ªÉ c√°c nh√£n gi·ªëng nhau n·∫±m g·∫ßn nhau

3. **Chia th√†nh Shards:**
   - Chia train th√†nh `num_clients * 2 = 10` m·∫£nh
   - M·ªói m·∫£nh c√≥ k√≠ch th∆∞·ªõc b·∫±ng nhau

4. **G√°n Shard cho Client:**
   - M·ªói Client nh·∫≠n **2 shards ng·∫´u nhi√™n**
   - ‚Üí M·ªói Client c√≥ ph√¢n ph·ªëi nh√£n kh√°c nhau (Non-IID)

### 8.3 K·∫øt qu·∫£

Sau khi ph√¢n chia:
```
Client 0: 397,989 samples
Client 1: 397,989 samples
Client 2: 397,989 samples
Client 3: 397,989 samples
Client 4: 397,988 samples
Test: 497,487 samples
```

---

## 9. PH·∫¶N 8-10: L∆ØU TR·ªÆ V√Ä VERIFICATION

### 9.1 L∆∞u d·ªØ li·ªáu ƒë√£ ph√¢n chia

```python
import numpy as np

output_dir = '../Output/data'
os.makedirs(output_dir, exist_ok=True)

for client_name, data in client_data.items():
    file_path = os.path.join(output_dir, f'{client_name}_data.npz')
    np.savez_compressed(
        file_path,
        X=data['X'],
        y=data['y']
    )
```

**Gi·∫£i th√≠ch:**
- **`.npz`**: Format file binary c·ªßa Numpy (n√©n, t·∫£i c·ª±c nhanh)
- M·ªói file ch·ª©a 2 arrays: `X` (features) v√† `y` (labels)

### 9.2 Verification (Ki·ªÉm tra)

```python
# Ki·ªÉm tra file ƒë√£ l∆∞u
for client_name in client_data.keys():
    file_path = os.path.join(output_dir, f'{client_name}_data.npz')
    if os.path.exists(file_path):
        file_size = os.path.getsize(file_path) / 1024**2  # MB
        print(f"‚úì {client_name}_data.npz ({file_size:.2f} MB)")
```

---

## 10. T√ìM T·∫ÆT TO√ÄN B·ªò QUY TR√åNH

### B∆∞·ªõc 1: Setup & Import
- Import th∆∞ vi·ªán chu·∫©n v√† module t·ª± vi·∫øt
- Set random seed ƒë·ªÉ reproducible

### B∆∞·ªõc 2: Load Configuration
- ƒê·ªçc file `training_config.yaml`
- L·∫•y c√°c tham s·ªë: s·ªë clients, test split ratio, ...

### B∆∞·ªõc 3: Load & Preprocess Dataset
- **Ki·ªÉm tra Cache**: N·∫øu ƒë√£ x·ª≠ l√Ω ‚Üí t·∫£i tr·ª±c ti·∫øp
- **N·∫øu ch∆∞a c√≥ Cache:**
  1. T·∫£i 169 file CSV
  2. Merge th√†nh 1 DataFrame
  3. **SHUFFLE** (c·ª±c k·ª≥ quan tr·ªçng!)
  4. Chia Train/Test (95%/5%)
  5. L∆∞u v√†o Cache

### B∆∞·ªõc 4: GSA Feature Selection
- D√πng 5% d·ªØ li·ªáu ƒë·ªÉ ch·∫°y GSA
- Ch·ªçn 22 features t·ªët nh·∫•t t·ª´ 46 features
- Gi·∫£m 52.2% k√≠ch th∆∞·ªõc

### B∆∞·ªõc 5: Encode Labels
- Chuy·ªÉn 34 t√™n t·∫•n c√¥ng ‚Üí S·ªë 0-33
- L∆∞u `label_encoder.pkl` v√† `labels.json`

### B∆∞·ªõc 6: Normalize Features
- D√πng MinMaxScaler ƒë∆∞a t·∫•t c·∫£ v·ªÅ [0, 1]
- L∆∞u `scaler.pkl` ƒë·ªÉ d√πng cho d·ªØ li·ªáu m·ªõi

### B∆∞·ªõc 7: Partition Data (Non-IID)
- Chia d·ªØ li·ªáu cho 5 Clients
- M·ªói Client c√≥ ph√¢n ph·ªëi nh√£n kh√°c nhau
- Test set chung cho t·∫•t c·∫£

### B∆∞·ªõc 8: Save & Verify
- L∆∞u d·ªØ li·ªáu d∆∞·ªõi d·∫°ng `.npz`
- Ki·ªÉm tra file ƒë√£ l∆∞u th√†nh c√¥ng

---

## 11. C√ÇU H·ªéI TH∆Ø·ªúNG G·∫∂P (FAQ)

**Q1: T·∫°i sao ph·∫£i Shuffle d·ªØ li·ªáu?**
- A: V√¨ d·ªØ li·ªáu g·ªëc c√≥ th·ªÉ s·∫Øp x·∫øp theo th·ªùi gian ho·∫∑c lo·∫°i t·∫•n c√¥ng. N·∫øu kh√¥ng shuffle, train/test s·∫Ω b·ªã l·ªách.

**Q2: T·∫°i sao ch·ªâ d√πng 5% d·ªØ li·ªáu cho GSA?**
- A: V√¨ GSA ch·∫°y r·∫•t l√¢u (30-60 ph√∫t). D√πng 5% v·ª´a ƒë·ªß ƒë·ªÉ t√¨m features t·ªët m√† kh√¥ng m·∫•t qu√° nhi·ªÅu th·ªùi gian.

**Q3: File `scaler.pkl` d√πng ƒë·ªÉ l√†m g√¨?**
- A: Khi c√≥ g√≥i tin m·∫°ng m·ªõi c·∫ßn d·ª± ƒëo√°n, b·∫°n PH·∫¢I d√πng scaler n√†y ƒë·ªÉ chu·∫©n h√≥a tr∆∞·ªõc khi ƒë∆∞a v√†o m√¥ h√¨nh.

**Q4: T·∫°i sao d√πng Non-IID thay v√¨ IID?**
- A: V√¨ trong th·ª±c t·∫ø, m·ªói thi·∫øt b·ªã IoT ch·ªâ th·∫•y m·ªôt ph·∫ßn l∆∞u l∆∞·ª£ng m·∫°ng. Non-IID m√¥ ph·ªèng ƒë√∫ng m√¥i tr∆∞·ªùng th·ª±c t·∫ø.

**Q5: File `.npz` l√† g√¨?**
- A: L√† format file binary c·ªßa Numpy, cho ph√©p l∆∞u v√† t·∫£i d·ªØ li·ªáu c·ª±c nhanh.

---

> [!IMPORTANT]
> **ƒêi·ªÉm quan tr·ªçng c·∫ßn nh·ªõ:**
> 1. **Shuffle** l√† b∆∞·ªõc KH√îNG TH·ªÇ thi·∫øu
> 2. **GSA** gi√∫p gi·∫£m 52% k√≠ch th∆∞·ªõc v√† tƒÉng ƒë·ªô ch√≠nh x√°c
> 3. **Scaler** ph·∫£i ƒë∆∞·ª£c l∆∞u l·∫°i ƒë·ªÉ d√πng cho d·ªØ li·ªáu m·ªõi
> 4. **Non-IID** m√¥ ph·ªèng ƒë√∫ng m√¥i tr∆∞·ªùng th·ª±c t·∫ø c·ªßa FL

---

**H·∫æT PH·∫¶N GI·∫¢I TH√çCH CHI TI·∫æT**

N·∫øu b·∫°n c√≥ c√¢u h·ªèi v·ªÅ b·∫•t k·ª≥ ph·∫ßn n√†o, h√£y h·ªèi t√¥i nh√©!

**T·∫†I SAO PH·∫¢I SHUFFLE?**

ƒê√¢y l√† c√¢u h·ªèi C·ª∞C K·ª≤ QUAN TR·ªåNG! H√£y hi·ªÉu k·ªπ:

**Tr∆∞·ªõc khi shuffle:**
```
File 1: DDoS-UDP (10,000 d√≤ng)
File 2: DDoS-TCP (15,000 d√≤ng)
File 3: Benign (20,000 d√≤ng)
File 4: Malware (8,000 d√≤ng)
...
```

N·∫øu b·∫°n **KH√îNG shuffle** m√† chia lu√¥n:
- **Train set** (95% ƒë·∫ßu): Ch·ªß y·∫øu l√† DDoS-UDP, DDoS-TCP
- **Test set** (5% cu·ªëi): Ch·ªß y·∫øu l√† Malware, Benign

**H·∫≠u qu·∫£:**
- M√¥ h√¨nh h·ªçc tr√™n DDoS ‚Üí Test tr√™n Malware ‚Üí **Accuracy = 0%**!

**Sau khi shuffle:**
```
D√≤ng 1: Malware
D√≤ng 2: Benign
D√≤ng 3: DDoS-UDP
D√≤ng 4: Benign
D√≤ng 5: DDoS-TCP
...
```

B√¢y gi·ªù Train v√† Test ƒë·ªÅu c√≥ **ph√¢n ph·ªëi ƒë·ªìng ƒë·ªÅu** c√°c lo·∫°i t·∫•n c√¥ng ‚Üí M√¥ h√¨nh h·ªçc ƒë√∫ng!

#### B∆Ø·ªöC 4: Chia Train/Test

```python
df_test = df_shuffled.head(int(len(df_shuffled) * test_size))
df_train = df_shuffled.tail(len(df_shuffled) - len(df_test))
```

**Gi·∫£i th√≠ch:**
- **`test_size = 0.05`**: L·∫•y 5% ƒë·∫ßu l√†m Test
- **`head()`**: L·∫•y n d√≤ng ƒë·∫ßu
- **`tail()`**: L·∫•y n d√≤ng cu·ªëi

**T·∫°i sao kh√¥ng d√πng `train_test_split()` c·ªßa sklearn?**
- C√≥ th·ªÉ d√πng! Nh∆∞ng c√°ch n√†y ƒë∆°n gi·∫£n h∆°n v√† ti·∫øt ki·ªám RAM.

#### B∆Ø·ªöC 5: L∆∞u v√†o Cache

```python
df_train.to_csv(train_file, index=False)
df_test.to_csv(test_file, index=False)
```

**Gi·∫£i th√≠ch:**
- **`index=False`**: Kh√¥ng l∆∞u c·ªôt index v√†o CSV
- L·∫ßn sau ch·∫°y l·∫°i ‚Üí CACHE HIT ‚Üí Nhanh h∆°n r·∫•t nhi·ªÅu!

### 4.4 K·∫øt qu·∫£ cu·ªëi c√πng

```
Train shape: (2,487,431, 47)  # 2.4 tri·ªáu d√≤ng
Test shape: (130,917, 47)     # 130 ngh√¨n d√≤ng
```

**Ph√¢n t√≠ch:**
- **47 c·ªôt** = 46 features + 1 label
- **T·ªïng**: 2,618,348 d√≤ng d·ªØ li·ªáu

---

## 5. PH·∫¶N 4: L·ªåC ƒê·∫∂C TR∆ØNG B·∫∞NG GSA

*(Ph·∫ßn n√†y s·∫Ω ƒë∆∞·ª£c vi·∫øt ti·∫øp...)*

---

**L∆ØU √ù:** T√†i li·ªáu n√†y ƒëang ƒë∆∞·ª£c vi·∫øt. T√¥i s·∫Ω ti·∫øp t·ª•c b·ªï sung c√°c ph·∫ßn c√≤n l·∫°i (GSA, Encode, Normalize, Partition). B·∫°n c√≥ th·ªÉ ƒë·ªçc ph·∫ßn n√†y tr∆∞·ªõc v√† cho t√¥i bi·∫øt c√≥ ch·ªó n√†o ch∆∞a r√µ kh√¥ng nh√©!
