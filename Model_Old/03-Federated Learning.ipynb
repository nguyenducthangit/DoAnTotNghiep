{"cells":[{"cell_type":"markdown","metadata":{"id":"Eu3Wq2LJ5ZXZ"},"source":["# 03 - Federated Learning"]},{"cell_type":"markdown","metadata":{"id":"DWH_oud25ZXa"},"source":["Define the available types of federated learning.\n","\n"," - 'STRATIFIED': Stratified sampling of the data. The data is split into a number of shards, and each shard is assigned to a client. The data is split in a stratified manner, meaning that the distribution of the labels is approximately the same in each shard.\n"," - 'LEAVE_ONE_OUT' - Each client is assigned a shard of data, each shard is missing one of the attack labels. Other clients in the network are exposed to the attack label, but the specific client is not. This demonstrates the ability of federated learning to protect against unknown attacks.\n"," - 'ONE_CLASS' - Each client is assigned a shard of data, each shard contains only one of the attack labels.\n"," - 'HALF_BENIGN' - Half of the clients are exposed to Benign data only, the other half are exposed to all data.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1749897589238,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"},"user_tz":-420},"id":"b7lHZAFc5ZXb"},"outputs":[],"source":["### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n","\n","SPLIT_AVAILABLE_METHODS = ['STRATIFIED','LEAVE_ONE_OUT', 'ONE_CLASS', 'HALF_BENIGN' ]\n","METHOD = 'STRATIFIED'\n","NUM_OF_STRATIFIED_CLIENTS = 10  # only applies to stratified method\n","NUM_OF_ROUNDS = 10              # Number of FL rounds\n"]},{"cell_type":"markdown","metadata":{"id":"Shgw7qC75ZXd"},"source":["The above test method in conjunction with the below classification selection will determine the number of clients.\n","\n","EG:\n","`STRATIFIED` with:\n"," - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n","\n","`LEAVE_ONE_OUT` with:\n"," - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n"," - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n"," - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n","\n","`ONE_CLASS` with:\n"," - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n"," - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n"," - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n","\n","`HALF_BENIGN` with:\n"," - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n"," - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n"," - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1749897589246,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"},"user_tz":-420},"id":"tLuY2D4Y5ZXe"},"outputs":[],"source":["individual_classifier = True\n","group_classifier = False\n","binary_classifier = False\n"]},{"cell_type":"markdown","metadata":{"id":"jNmFr3zE5ZXe"},"source":["Include the defines for the dataframe columns and the attack labels and their mappings"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":114881,"status":"ok","timestamp":1749897704144,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"},"user_tz":-420},"id":"E4SpVjkG56Dn","outputId":"d6bf6df3-f4ad-4235-d8ff-d1c235ecbf3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":42,"status":"ok","timestamp":1749897704148,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"},"user_tz":-420},"id":"E5j4Y8vPVrx7"},"outputs":[],"source":["import sys\n","sys.path.append('/content/drive/MyDrive/Do_An_Tot_Nghiep/CICIoT2023/')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1724,"status":"ok","timestamp":1749897705834,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"},"user_tz":-420},"id":"kfvPcF405ZXe"},"outputs":[],"source":["from enum import Enum\n","from includes import *\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":91217,"status":"ok","timestamp":1749898995869,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"},"user_tz":-420},"id":"7WFS1dVocueX","outputId":"41252492-1468-4a8a-bad2-8a9f99f0e761"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: numpy 1.26.4\n","Uninstalling numpy-1.26.4:\n","  Successfully uninstalled numpy-1.26.4\n","Found existing installation: tensorflow 2.19.0\n","Uninstalling tensorflow-2.19.0:\n","  Successfully uninstalled tensorflow-2.19.0\n","Found existing installation: torch 2.0.1\n","Uninstalling torch-2.0.1:\n","  Successfully uninstalled torch-2.0.1\n","Found existing installation: torchvision 0.15.2\n","Uninstalling torchvision-0.15.2:\n","  Successfully uninstalled torchvision-0.15.2\n","Found existing installation: flwr 1.5.0\n","Uninstalling flwr-1.5.0:\n","  Successfully uninstalled flwr-1.5.0\n","Found existing installation: scikit-learn 1.2.2\n","Uninstalling scikit-learn-1.2.2:\n","  Successfully uninstalled scikit-learn-1.2.2\n","Collecting numpy==1.23.5\n","  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n","Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n","umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n","dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n","yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n","shap 0.47.2 requires scikit-learn, which is not installed.\n","peft 0.15.2 requires torch>=1.13.0, which is not installed.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n","hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n","accelerate 1.7.0 requires torch>=2.0.0, which is not installed.\n","librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n","tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n","libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n","sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n","pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n","albumentations 2.0.8 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.22 which is incompatible.\n","jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n","thinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.22 which is incompatible.\n","chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n","scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n","treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n","gradio 5.31.0 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.22 which is incompatible.\n","imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n","ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n","bigframes 2.5.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n","blosc2 3.3.4 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n","albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n","jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.5\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"11242b2c66d8467ca0fdb7dd37c032be"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==1.18.0 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.18.1, 2.19.0rc0, 2.19.0)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==1.18.0\u001b[0m\u001b[31m\n","\u001b[0mCollecting torch==2.0.1\n","  Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n","Collecting torchvision==0.15.2\n","  Using cached torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.14.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (2.32.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (11.2.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.2.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2025.4.26)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n","Using cached torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n","Using cached torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl (6.0 MB)\n","Installing collected packages: torch, torchvision\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.19 requires scikit-learn, which is not installed.\n","sentence-transformers 4.1.0 requires scikit-learn, which is not installed.\n","torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed torch-2.0.1 torchvision-0.15.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nvfuser","torch"]},"id":"c9ebe29c0b2646969b10ac4c81ba7416"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting flwr==1.18.0 (from flwr[simulation]==1.18.0)\n","  Downloading flwr-1.18.0-py3-none-any.whl.metadata (15 kB)\n","Collecting cryptography<45.0.0,>=44.0.1 (from flwr==1.18.0->flwr[simulation]==1.18.0)\n","  Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n","Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr==1.18.0->flwr[simulation]==1.18.0) (1.72.1)\n","Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr==1.18.0->flwr[simulation]==1.18.0) (0.0.2)\n","Collecting numpy<3.0.0,>=1.26.0 (from flwr==1.18.0->flwr[simulation]==1.18.0)\n","  Using cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n","Collecting pathspec<0.13.0,>=0.12.1 (from flwr==1.18.0->flwr[simulation]==1.18.0)\n","  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n","Collecting protobuf<5.0.0,>=4.21.6 (from flwr==1.18.0->flwr[simulation]==1.18.0)\n","  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr==1.18.0->flwr[simulation]==1.18.0) (3.23.0)\n","Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr==1.18.0->flwr[simulation]==1.18.0) (6.0.2)\n","Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr==1.18.0->flwr[simulation]==1.18.0) (2.32.3)\n","Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr==1.18.0->flwr[simulation]==1.18.0) (13.9.4)\n","Collecting tomli<3.0.0,>=2.0.1 (from flwr==1.18.0->flwr[simulation]==1.18.0)\n","  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Collecting tomli-w<2.0.0,>=1.0.0 (from flwr==1.18.0->flwr[simulation]==1.18.0)\n","  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n","Collecting typer<0.13.0,>=0.12.5 (from flwr==1.18.0->flwr[simulation]==1.18.0)\n","  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n","Collecting ray==2.31.0 (from flwr[simulation]==1.18.0)\n","  Downloading ray-2.31.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (13 kB)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]==1.18.0) (8.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]==1.18.0) (3.18.0)\n","Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]==1.18.0) (4.24.0)\n","Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]==1.18.0) (1.1.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]==1.18.0) (24.2)\n","Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]==1.18.0) (1.3.2)\n","Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]==1.18.0) (1.6.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr==1.18.0->flwr[simulation]==1.18.0) (1.17.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr==1.18.0->flwr[simulation]==1.18.0) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr==1.18.0->flwr[simulation]==1.18.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr==1.18.0->flwr[simulation]==1.18.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr==1.18.0->flwr[simulation]==1.18.0) (2025.4.26)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr==1.18.0->flwr[simulation]==1.18.0) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr==1.18.0->flwr[simulation]==1.18.0) (2.19.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr==1.18.0->flwr[simulation]==1.18.0) (4.14.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr==1.18.0->flwr[simulation]==1.18.0) (1.5.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr==1.18.0->flwr[simulation]==1.18.0) (2.22)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr==1.18.0->flwr[simulation]==1.18.0) (0.1.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]==1.18.0) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]==1.18.0) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]==1.18.0) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]==1.18.0) (0.25.1)\n","Downloading flwr-1.18.0-py3-none-any.whl (540 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.0/540.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ray-2.31.0-cp311-cp311-manylinux2014_x86_64.whl (66.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached numpy-2.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n","Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n","Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n","Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tomli-w, tomli, protobuf, pathspec, numpy, cryptography, typer, ray, flwr\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.23.5\n","    Uninstalling numpy-1.23.5:\n","      Successfully uninstalled numpy-1.23.5\n","  Attempting uninstall: cryptography\n","    Found existing installation: cryptography 41.0.7\n","    Uninstalling cryptography-41.0.7:\n","      Successfully uninstalled cryptography-41.0.7\n","  Attempting uninstall: typer\n","    Found existing installation: typer 0.16.0\n","    Uninstalling typer-0.16.0:\n","      Successfully uninstalled typer-0.16.0\n","  Attempting uninstall: ray\n","    Found existing installation: ray 2.6.3\n","    Uninstalling ray-2.6.3:\n","      Successfully uninstalled ray-2.6.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n","fastai 2.7.19 requires scikit-learn, which is not installed.\n","umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n","dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n","yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n","shap 0.47.2 requires scikit-learn, which is not installed.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n","hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n","librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n","tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n","libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n","sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n","albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.22 which is incompatible.\n","grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n","thinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.22 which is incompatible.\n","pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.0 which is incompatible.\n","pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\n","gradio 5.31.0 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.22 which is incompatible.\n","cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.0 which is incompatible.\n","ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed cryptography-44.0.3 flwr-1.18.0 numpy-2.3.0 pathspec-0.12.1 protobuf-4.25.8 ray-2.31.0 tomli-2.2.1 tomli-w-1.2.0 typer-0.12.5\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["_openssl","cryptography","google","numpy"]},"id":"64356e60402b46aaa918c456154894a8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting scikit-learn==1.2.2\n","  Using cached scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (2.3.0)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\n","Using cached scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n","Installing collected packages: scikit-learn\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed scikit-learn-1.2.2\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"]}],"source":["# Gỡ các thư viện (nếu còn sót)\n","!pip3 uninstall -y numpy tensorflow torch torchvision flwr scikit-learn\n","\n","# Cài numpy trước để tránh lỗi tương thích\n","!pip3 install numpy==1.23.5\n","\n","# Cài các thư viện còn lại\n","!pip3 install tensorflow==1.18.0\n","!pip3 install torch==2.0.1 torchvision==0.15.2\n","!pip3 install \"flwr[simulation]==1.18.0\"\n","!pip3 install scikit-learn==1.2.2\n","!pip3 install pandas matplotlib"]},{"cell_type":"markdown","metadata":{"id":"ahbV3kbf5ZXf"},"source":["##  Imports"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":40521,"status":"error","timestamp":1749899194397,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"},"user_tz":-420},"id":"vweRw48oc3g3","outputId":"359ac911-f472-4bcf-df77-50aa95e25167"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.3.0)\n","Collecting tensorflow\n","  Using cached tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.0.1)\n","Requirement already satisfied: flwr in /usr/local/lib/python3.11/dist-packages (1.18.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.8)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.1)\n","Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n","Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n","Collecting numpy\n","  Using cached numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n","Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.45.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch) (3.31.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch) (18.1.8)\n","Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (44.0.3)\n","Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.0.2)\n","Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.1)\n","Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (3.23.0)\n","Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr) (6.0.2)\n","Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (13.9.4)\n","Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr) (2.2.1)\n","Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr) (1.2.0)\n","Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr) (0.12.5)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr) (1.17.1)\n","Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n","Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr) (2.19.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (8.2.1)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr) (1.5.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr) (2.22)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr) (0.1.2)\n","Using cached tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n","Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy, tensorflow\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.3.0\n","    Uninstalling numpy-2.3.0:\n","      Successfully uninstalled numpy-2.3.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n","albumentations 2.0.8 requires pydantic>=2.9.2, but you have pydantic 1.10.22 which is incompatible.\n","mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n","tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n","thinc 8.3.6 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.10.22 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n","tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n","gradio 5.31.0 requires pydantic<2.12,>=2.0, but you have pydantic 1.10.22 which is incompatible.\n","imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n","ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-2.1.3 tensorflow-2.19.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy","tensorflow"]},"id":"0a0c149fc5a04111ac81cbf48687acc9"}},"metadata":{}},{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'warn_deprecated_feature' from 'flwr.common.logger' (/usr/local/lib/python3.11/dist-packages/flwr/common/logger.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-927773902>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mflwr\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NumPy version:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flwr/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflwr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpackage_version\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_package_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flwr/common/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessageType\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMessageType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMessageTypeLegacy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMessageTypeLegacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContext\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mgrpc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGRPC_MAX_MESSAGE_LENGTH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flwr/common/context.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataclasses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecordDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flwr/common/record/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrayrecord\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mArrayRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParametersRecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfigrecord\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfigsRecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconversion_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_from_numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetricrecord\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMetricRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetricsRecord\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrecorddict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecordDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRecordSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flwr/common/record/conversion_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn_deprecated_feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNDArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrayrecord\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'warn_deprecated_feature' from 'flwr.common.logger' (/usr/local/lib/python3.11/dist-packages/flwr/common/logger.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["%pip install numpy tensorflow torch flwr\n","\n","import numpy as np\n","import tensorflow as tf\n","import torch\n","import flwr as fl\n","\n","print(\"NumPy version:\", np.__version__)\n","print(\"TensorFlow version:\", tf.__version__)\n","print(\"PyTorch version:\", torch.__version__)\n","print(\"Flower version:\", fl.__version__)"]},{"source":["# # Gỡ các thư viện (nếu còn sót)\n","# !pip3 uninstall -y numpy tensorflow torch torchvision flwr scikit-learn pandas matplotlib\n","\n","# # Cài numpy trước để tránh lỗi tương thích\n","# !pip3 install numpy==1.22.4\n","\n","# # Cài các thư thư viện còn lại với các phiên bản tương thích\n","# !pip3 install tensorflow==2.11.0\n","# !pip3 install torch==2.0.1 torchvision==0.15.2\n","# !pip3 install \"flwr[simulation]==1.5.0\"\n","# !pip3 install scikit-learn==1.2.2\n","# !pip3 install pandas==1.5.3 matplotlib==3.7.1"],"cell_type":"code","metadata":{"id":"h_Y_vwfGFDn3"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vN0A0PZn5ZXg","executionInfo":{"status":"aborted","timestamp":1749898323238,"user_tz":-420,"elapsed":734290,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["%pip install pandas\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import sklearn\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import preprocessing\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import accuracy_score\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from flwr.common import Metrics\n","from torch.utils.data import DataLoader, random_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qjJckTe5ZXg","executionInfo":{"status":"aborted","timestamp":1749898323250,"user_tz":-420,"elapsed":734294,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["print(\"flwr\", fl.__version__)\n","print(\"numpy\", np.__version__)\n","print(\"torch\", torch.__version__)\n","\n","DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Training on {DEVICE}\")"]},{"cell_type":"markdown","metadata":{"id":"FyYXTMz-5ZXh"},"source":["## Load the Dataset"]},{"cell_type":"markdown","metadata":{"id":"AjVdUs4v5ZXi"},"source":["## Training data"]},{"cell_type":"markdown","metadata":{"id":"gPR5uiov5ZXi"},"source":["Either read the training pickle file if it exists, or process the dataset from scratch."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWZHuA3W5ZXi","executionInfo":{"status":"aborted","timestamp":1749898323274,"user_tz":-420,"elapsed":734310,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["import pandas as pd\n","import os\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","\n","# Định nghĩa thư mục chứa dữ liệu\n","DATASET_DIRECTORY = '/content/drive/MyDrive/Do_An_Tot_Nghiep/dataset/CICIoT2023'  # Thay đổi nếu cần\n","# DATASET_DIRECTORY = '/content/drive/MyDrive/HOCVIEN/29. NCKH_NCS/Dataset'\n","# DATASET_DIRECTORY = './Dataset/'\n","# try:\n","#     from google.colab import drive\n","#     drive.mount('/content/drive')\n","# except ImportError:\n","#     print(\"Not running in Colab, assuming local file system access.\")\n","\n","# Kiểm tra xem tệp 'training_data.pkl' có tồn tại không\n","if os.path.isfile('training_data.pkl'):\n","    print(\"File exists, loading data...\")\n","    train_df = pd.read_pickle('training_data.pkl')\n","    print(\"Training data loaded from pickle file.\")\n","else:\n","    # Lấy danh sách các tệp CSV trong thư mục\n","    try:\n","        df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n","        df_sets.sort()\n","    except FileNotFoundError:\n","        print(f\"Directory not found: {DATASET_DIRECTORY}\")\n","        raise\n","\n","    # Chia tập huấn luyện (80%) và tập kiểm tra (20%)\n","    training_sets = df_sets[:int(len(df_sets) * 0.8)]\n","    test_sets = df_sets[int(len(df_sets) * 0.8):]\n","\n","    # In số lượng tệp\n","    print('Training sets: {}'.format(len(training_sets)))\n","    print('Test sets: {}'.format(len(test_sets)))\n","\n","    # Kiểm tra xem training_sets có rỗng không\n","    if not training_sets:\n","        print(\"No CSV files found in the directory. Please check DATASET_DIRECTORY.\")\n","        raise FileNotFoundError(\"No CSV files found.\")\n","\n","    # Đọc và nối các tập huấn luyện\n","    dfs = []\n","    print(\"Reading training data...\")\n","    for train_set in tqdm(training_sets):\n","        file_path = os.path.join(DATASET_DIRECTORY, train_set)  # Sử dụng os.path.join để xử lý đường dẫn\n","        try:\n","            df_new = pd.read_csv(file_path)\n","            dfs.append(df_new)\n","        except FileNotFoundError:\n","            print(f\"File not found: {file_path}. Skipping...\")\n","            continue\n","        except Exception as e:\n","            print(f\"Error reading {file_path}: {str(e)}. Skipping...\")\n","            continue\n","\n","    # Kiểm tra xem có dữ liệu nào được đọc không\n","    if not dfs:\n","        print(\"No valid CSV files were loaded. Please check the files in the directory.\")\n","        raise ValueError(\"No data loaded.\")\n","\n","    # Nối các dataframe\n","    train_df = pd.concat(dfs, ignore_index=True)\n","    print(\"Complete training data size: {}\".format(train_df.shape))\n","\n","    # Ánh xạ cột 'label' sang dict_34_classes (giả định đã được định nghĩa)\n","    try:\n","        train_df['label'] = train_df['label'].map(dict_34_classes)\n","    except NameError:\n","        print(\"Error: dict_34_classes is not defined. Please define the mapping dictionary.\")\n","        raise\n","\n","    # Chia tập huấn luyện với tỉ lệ TRAIN_SIZE\n","    TRAIN_SIZE = 0.01\n","    print(f\"Splitting the data into {TRAIN_SIZE * 100}%\")\n","\n","    try:\n","        X_train, X_test, y_train, y_test = train_test_split(\n","            train_df[X_columns], train_df[y_column],\n","            test_size=(1 - TRAIN_SIZE), random_state=42, stratify=train_df[y_column]\n","        )\n","    except NameError:\n","        print(\"Error: X_columns or y_column is not defined. Please define these variables.\")\n","        raise\n","\n","    # Kết hợp lại X_train và y_train\n","    train_df = pd.concat([X_train, y_train], axis=1)\n","\n","    # Dọn dẹp biến không dùng\n","    del X_train, y_train, X_test, y_test\n","\n","    # Lưu dữ liệu vào tệp pickle\n","    print(\"Writing training data to pickle file...\")\n","    try:\n","        train_df.to_pickle('training_data.pkl')\n","    except Exception as e:\n","        print(f\"Error saving pickle file: {str(e)}\")\n","\n","# In kích thước tập huấn luyện cuối cùng\n","print(\"Training data size: {}\".format(train_df.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R40h7lR05ZXi","executionInfo":{"status":"aborted","timestamp":1749898323279,"user_tz":-420,"elapsed":734305,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["# show the unique values counts in the label column for train_df\n","print(\"Counts of attacks in train_df:\")\n","print(train_df['label'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUw_flQh5ZXj","executionInfo":{"status":"aborted","timestamp":1749898323513,"user_tz":-420,"elapsed":734530,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["train_df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vR00qKT4VryA","executionInfo":{"status":"aborted","timestamp":1749898323516,"user_tz":-420,"elapsed":734518,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["train_df"]},{"cell_type":"markdown","metadata":{"id":"kOZb7hVX5ZXj"},"source":["---\n","## Test Data\n","Concat the test data into a single dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JygmVxy85ZXj","executionInfo":{"status":"aborted","timestamp":1749898323520,"user_tz":-420,"elapsed":734513,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["import pickle\n","\n","# Check if the pickle file exists\n","testing_data_pickle_file = 'testing_data.pkl'\n","\n","try:\n","    if os.path.isfile(testing_data_pickle_file):\n","        print(f\"File {testing_data_pickle_file} exists, loading data...\")\n","        test_df = pd.read_pickle(testing_data_pickle_file)\n","        print(\"Test data loaded from pickle file.\")\n","    else:\n","        raise FileNotFoundError(f\"File {testing_data_pickle_file} does not exist.\")\n","except (EOFError, pickle.UnpicklingError) as e:\n","    print(f\"Error loading pickle file: {e}. Reconstructing data...\")\n","    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n","    df_sets.sort()\n","    test_sets = df_sets[int(len(df_sets) * 0.8):]\n","\n","    # Read and concatenate test data\n","    dfs = []\n","    print(\"Reading test data...\")\n","    for test_set in tqdm(test_sets):\n","        try:\n","            df_new = pd.read_csv(os.path.join(DATASET_DIRECTORY, test_set))\n","            dfs.append(df_new)\n","        except FileNotFoundError:\n","            print(f\"File not found: {test_set}. Skipping...\")\n","        except Exception as e:\n","            print(f\"Error reading {test_set}: {e}. Skipping...\")\n","    test_df = pd.concat(dfs, ignore_index=True)\n","\n","    # Map labels to integers\n","    try:\n","        test_df['label'] = test_df['label'].map(dict_34_classes)\n","    except KeyError as e:\n","        print(f\"Error mapping labels: {e}. Check dict_34_classes.\")\n","        raise\n","\n","    # Save the test data to a pickle file\n","    try:\n","        test_df.to_pickle(testing_data_pickle_file)\n","        print(f\"Test data saved to {testing_data_pickle_file}.\")\n","    except Exception as e:\n","        print(f\"Error saving pickle file: {e}\")\n","\n","print(\"Testing data size: {}\".format(test_df.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4K4GaYg5ZXj","executionInfo":{"status":"aborted","timestamp":1749898323526,"user_tz":-420,"elapsed":734510,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["print(\"Number of rows in train_df: {}\".format(len(train_df)))\n","print(\"Number of rows in test_df: {}\".format(len(test_df)))\n","\n","train_size = len(train_df)\n","test_size = len(test_df)"]},{"cell_type":"markdown","metadata":{"id":"1rhsFnf_5ZXk"},"source":["---\n","# Scale the test and train data"]},{"cell_type":"markdown","metadata":{"id":"tkflFq9A5ZXk"},"source":["### Scale the training data input features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KaNMGFBX5ZXk","executionInfo":{"status":"aborted","timestamp":1749898323608,"user_tz":-420,"elapsed":734591,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["X_columns = [col for col in train_df.columns if col != 'label']\n","y_column = 'label'\n","##\n","scaler = StandardScaler()\n","train_df[X_columns] = scaler.fit_transform(train_df[X_columns])"]},{"cell_type":"markdown","metadata":{"id":"Z0PWkRqD5ZXl"},"source":["### Scale the testing data input features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdpFklaE5ZXl","executionInfo":{"status":"aborted","timestamp":1749898323810,"user_tz":-420,"elapsed":734792,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["test_df[X_columns] = scaler.fit_transform(test_df[X_columns])"]},{"cell_type":"markdown","metadata":{"id":"1lON9WUp5ZXl"},"source":["---\n","# Define the classification problem - (2 classes, 8 classes or 34 classes)\n","Change the following cell to select the classification type\n","\n","If the METHOD == STRATIFIED, then we can use any classifier\n","If the METHOD == ATTACK_GROUP then we must use Group Classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7C-bHQTH5ZXl","executionInfo":{"status":"aborted","timestamp":1749898323817,"user_tz":-420,"elapsed":734790,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["\n","class_size_map = {2: \"Binary\", 8: \"Group\", 34: \"Individual\"}\n","\n","if group_classifier:\n","    print(\"Group 8 Class Classifier... - Adjusting labels in test and train dataframes\")\n","    # Map y column to the dict_7_classes values\n","    test_df['label'] = test_df['label'].map(dict_8_classes)\n","    train_df['label'] = train_df['label'].map(dict_8_classes)\n","    class_size = \"8\"\n","\n","elif binary_classifier:\n","    print(\"Binary 2 Class Classifier... - Adjusting labels in test and train dataframes\")\n","    # Map y column to the dict_2_classes values\n","    test_df['label'] = test_df['label'].map(dict_2_classes)\n","    train_df['label'] = train_df['label'].map(dict_2_classes)\n","    class_size = \"2\"\n","\n","else:\n","    print (\"Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\")\n","    class_size = \"34\"\n"]},{"cell_type":"markdown","metadata":{"id":"7k_nPpRH5ZXl"},"source":["---\n","# Split the Training Data into partitions for the Federated Learning clients depending on the test required\n","As a reminder:\n","\n","`STRATIFIED` with:\n"," - `ALL TYPES` - Results in `NUM_OF_STRATIFIED_CLIENTS` clients. Each client will have a stratified sample of the data.\n","\n","`LEAVE_ONE_OUT` with:\n"," - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 32 attack labels.\n"," - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 6 attack groups.\n"," - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels.\n","\n","`ONE_CLASS` with:\n"," - `individual_classifier` - Results in 33 clients. Each client will have benign traffic and 1 attack label.\n"," - `group_classifier` - Results in 7 clients. Each client will have benign traffic and 1 attack groups.\n"," - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n","\n","`HALF_BENIGN` with:\n"," - `individual_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 33 malicious attack labels.\n"," - `group_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and 7 malicious attack groups.\n"," - `binary_classifier` - Results in 10 clients. Five clients will have benign traffic only and the other will have Benign and malicious attack labels. - SAME AS LEAVE_ONE_OUT for binary classifier\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0GjjGsp5ZXm","executionInfo":{"status":"aborted","timestamp":1749898323820,"user_tz":-420,"elapsed":734785,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold\n","\n","# Define fl_X_train and fl_y_train\n","fl_X_train = []\n","fl_y_train = []\n","\n","client_df = pd.DataFrame()\n","\n","if METHOD == 'STRATIFIED':\n","    print(f\"{Colours.YELLOW.value}STRATIFIED METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n","    # We are going to split the training data into 'NUM_OF_STRATIFIED_CLIENTS' smaller groups using StratifiedKFold\n","    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n","    for train_index, test_index in skf.split(train_df[X_columns], train_df[y_column]):\n","        fl_X_train.append(train_df[X_columns].iloc[test_index])\n","        fl_y_train.append(train_df[y_column].iloc[test_index])\n","\n","elif METHOD == 'LEAVE_ONE_OUT':\n","    print(f\"{Colours.YELLOW.value}LEAVE_ONE_OUT METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n","\n","    if individual_classifier or group_classifier:\n","        # Set the number of splits required to the number of classes - 1\n","        num_splits = int(class_size) - 1\n","    else:\n","        # For binary classifier, set the number of splits to 10\n","        num_splits = 10\n","\n","    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n","\n","    # When creating the clients, we will remove one attack class from the training data\n","    # For the binary classifier, evey other client will have the benign class removed\n","    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n","        if binary_classifier:\n","            print(f\"i: {i} = i % 2 = {i % 2}\")\n","            if i % 2 == 0:\n","                print(\"Benign only\")\n","                # Create a new dataframe for the client data with only benign traffic\n","                client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != 1]], ignore_index=True)\n","                fl_X_train.append(client_df[X_columns])\n","                fl_y_train.append(client_df[y_column])\n","            else:\n","                print(\"Both\")\n","                # Create a new dataframe for the client data\n","                fl_X_train.append(train_df[X_columns].iloc[test_index])\n","                fl_y_train.append(train_df[y_column].iloc[test_index])\n","        else:\n","            # Create a new dataframe for the client data\n","            client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != i+1]], ignore_index=True)\n","            fl_X_train.append(client_df[X_columns])\n","            fl_y_train.append(client_df[y_column])\n","\n","elif METHOD == 'ONE_CLASS':\n","    print(f\"{Colours.YELLOW.value}ONE_CLASS METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n","    # Each client only has one attack class in their training data along with the Benign data\n","\n","    if individual_classifier or group_classifier:\n","        # Set the number of splits required to the number of classes - 1\n","        num_splits = int(class_size) - 1\n","    else:\n","        # For binary classifier, set the number of splits to 10\n","        num_splits = 10\n","\n","    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n","\n","    # When creating the clients, we will only add the benign data and the attack class for that client\n","    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n","        if binary_classifier:\n","            print(f\"i: {i} = i % 2 = {i % 2}\")\n","            if i % 2 == 0:\n","                print(\"Benign only\")\n","                # Create a new dataframe for the client data with only benign traffic\n","                client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] != 1]], ignore_index=True)\n","                fl_X_train.append(client_df[X_columns])\n","                fl_y_train.append(client_df[y_column])\n","            else:\n","                print(\"Both\")\n","                # Create a new dataframe for the client data\n","                fl_X_train.append(train_df[X_columns].iloc[test_index])\n","                fl_y_train.append(train_df[y_column].iloc[test_index])\n","        else:\n","            # Create a new dataframe for the client data\n","            client_df = pd.concat([train_df.iloc[test_index][(train_df[y_column] == 0) | (train_df[y_column] == i+1)]], ignore_index=True)\n","            fl_X_train.append(client_df[X_columns])\n","            fl_y_train.append(client_df[y_column])\n","\n","elif METHOD == 'HALF_BENIGN':\n","    print(f\"{Colours.YELLOW.value}HALF_BENIGN METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n","\n","    num_splits = 10\n","\n","    # Split into 10 client data\n","    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n","\n","    # For i % 2 == 0, add only benign data\n","    # For i % 2 == 1, add all data\n","    for i, (train_index, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n","        if i % 2 == 0:\n","            print(\"Benign only\")\n","            # Create a new dataframe for the client data with only benign traffic\n","            client_df = pd.concat([train_df.iloc[test_index][train_df[y_column] == 0]], ignore_index=True)\n","            fl_X_train.append(client_df[X_columns])\n","            fl_y_train.append(client_df[y_column])\n","        else:\n","            print(\"All Classes\")\n","            fl_X_train.append(train_df[X_columns].iloc[test_index])\n","            fl_y_train.append(train_df[y_column].iloc[test_index])\n","else:\n","    print(f\"{Colours.RED.value}ERROR: Method {METHOD} not recognised{Colours.NORMAL.value}\")\n","    exit()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkcbt_k35ZXm","executionInfo":{"status":"aborted","timestamp":1749898323837,"user_tz":-420,"elapsed":734789,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["NUM_OF_CLIENTS = len(fl_X_train)\n","\n","for i in range(len(fl_X_train)):\n","    # Show the unique values in the y column\n","    (f\"Client ID: {i}\")\n","    print(f\"fl_X_train[{i}].shape: {fl_X_train[i].shape}\")\n","    print(f\"fl_y_train[{i}].value_counts():\\n{fl_y_train[i].value_counts()}\")\n","    print(f\"fl_y_train[{i}].unique(): {fl_y_train[i].unique()}\\n\")\n","\n","# Check that fl_X_train[0] and fl_X_train[1] contain different data\n","print(f\"fl_X_train[0].equals(fl_X_train[1]): {fl_X_train[0].equals(fl_X_train[1])}\")"]},{"cell_type":"markdown","metadata":{"id":"FEp4UP3S5ZXm"},"source":["Prepare an output directory where we can store the results of the federated learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rt9hdrpb5ZXn","executionInfo":{"status":"aborted","timestamp":1749898323878,"user_tz":-420,"elapsed":734829,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["# Create an \"Output\" directory if it doesnt exist already\n","if not os.path.exists(\"Output\"):\n","    os.makedirs(\"Output\")\n","\n","sub_dir_name = f\"train_size-{train_size}\"\n","\n","# if sub_dir_name does not exist, create it\n","if not os.path.exists(f\"Output/{sub_dir_name}\"):\n","    os.makedirs(f\"Output/{sub_dir_name}\")\n","\n","test_directory_name = f\"{METHOD}_Classifier-{class_size}_Clients-{NUM_OF_CLIENTS}\"\n","\n","# Create an \"Output/{METHOD}-{NUM_OF_CLIENTS}-{NUM_OF_ROUNDS}\" directory if it doesnt exist already\n","if not os.path.exists(f\"Output/{sub_dir_name}/{test_directory_name}\"):\n","    os.makedirs(f\"Output/{sub_dir_name}/{test_directory_name}\")\n","\n","# Ensure the directory is empty\n","for file in os.listdir(f\"Output/{sub_dir_name}/{test_directory_name}\"):\n","    file_path = os.path.join(f\"Output/{sub_dir_name}/{test_directory_name}\", file)\n","    if os.path.isfile(file_path):\n","        os.unlink(file_path)\n","\n","# Original training size is the sum of all the fl_X_train sizes\n","original_training_size = 0\n","for i in range(len(fl_X_train)):\n","    original_training_size += fl_X_train[i].shape[0]\n","\n","# Write this same info to the output directory/Class Split Info.txt\n","with open(f\"Output/{sub_dir_name}/{test_directory_name}/Class Split Info.txt\", \"w\") as f:\n","    for i in range(len(fl_X_train)):\n","        f.write(f\"Client ID: {i}\\n\")\n","        f.write(f\"fl_X_train.shape: {fl_X_train[i].shape}\\n\")\n","        f.write(f\"Training data used {original_training_size}\")\n","        f.write(f\"fl_y_train.value_counts():\\n{fl_y_train[i].value_counts()}\\n\")\n","        f.write(f\"fl_y_train.unique(): {fl_y_train[i].unique()}\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"lhNIzYTZ5ZXn"},"source":["### Convert the training dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DpoUL0Q85ZXn","executionInfo":{"status":"aborted","timestamp":1749898323889,"user_tz":-420,"elapsed":734839,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["# Convert the testing daya to X_test and y_test ndarrays\n","X_test = test_df[X_columns].to_numpy()\n","y_test = test_df[y_column].to_numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W4QBuakw5ZXn","executionInfo":{"status":"aborted","timestamp":1749898323896,"user_tz":-420,"elapsed":734846,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["num_unique_classes = len(train_df[y_column].unique())\n","\n","train_df_shape = train_df.shape\n","test_df_shape = test_df.shape\n","\n","# We are now done with the train_df and test_df dataframes, so we can delete them to free up memory\n","del train_df\n","del test_df\n","del client_df"]},{"cell_type":"markdown","metadata":{"id":"L15udsEn5ZXn"},"source":["---\n","### Data check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"on0GTYwp5ZXn","executionInfo":{"status":"aborted","timestamp":1749898323899,"user_tz":-420,"elapsed":734840,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["print(\"NUM_CLIENTS:\", NUM_OF_CLIENTS)\n","\n","print(\"NUM_ROUNDS:\", NUM_OF_ROUNDS)\n","print()\n","\n","\n","print(\"Original training size: {}\".format(original_training_size))\n","\n","\n","print(\"Checking training data split groups\")\n","for i in range(len(fl_X_train)):\n","    print(i, \":\", \"X Shape\", fl_X_train[i].shape, \"Y Shape\", fl_y_train[i].shape)\n","\n","\n","# Print the sizes of X_test and y_test\n","print(\"\\nChecking testing data\")\n","print(\"X_test size: {}\".format(X_test.shape))\n","print(\"y_test size: {}\".format(y_test.shape))\n","\n","print(\"\\nDeploy Simulation\")"]},{"cell_type":"markdown","metadata":{"id":"WP_IyhLk5ZXo"},"source":["----\n","# Federated Learning\n","## Import the libraries and print the versions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h43yzAry5ZXz","executionInfo":{"status":"aborted","timestamp":1749898323904,"user_tz":-420,"elapsed":734844,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["import os\n","import flwr as fl\n","import numpy as np\n","import tensorflow as tf\n","\n","# Make TensorFlow log less verbose\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Dropout\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zN8rYdqa5ZXz"},"source":["Define the Client and Server code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"32XOxLnv5ZX0","executionInfo":{"status":"aborted","timestamp":1749898324117,"user_tz":-420,"elapsed":735049,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["import os\n","import flwr as fl\n","import numpy as np\n","import tensorflow as tf\n","\n","print('scikit-learn {}.'.format(sklearn.__version__))\n","print(\"flwr\", fl.__version__)\n","print(\"numpy\", np.__version__)\n","print(\"tf\", tf.__version__)\n","# Make TensorFlow log less verbose\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import Dropout\n","\n","import datetime\n","\n","client_evaluations = []\n","\n","class NumpyFlowerClient(fl.client.NumPyClient):\n","    def __init__(self, cid, model, train_data, train_labels):\n","        self.model = model\n","        self.cid = cid\n","        self.train_data = train_data\n","        self.train_labels = train_labels\n","\n","    def get_parameters(self, config):\n","        return self.model.get_weights()\n","\n","    def fit(self, parameters, config):\n","        self.model.set_weights(parameters)\n","        print (\"Client \", self.cid, \"Training...\")\n","        self.model.fit(self.train_data, self.train_labels, epochs=5, batch_size=32)\n","        print (\"Client \", self.cid, \"Training complete...\")\n","        return self.model.get_weights(), len(self.train_data), {}\n","\n","    def evaluate(self, parameters, config):\n","        self.model.set_weights(parameters)\n","        print (\"Client \", self.cid, \"Evaluating...\")\n","        loss, accuracy = self.model.evaluate(self.train_data, self.train_labels, batch_size=32)\n","        print(f\"{Colours.YELLOW.value}Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}{Colours.NORMAL.value}\")\n","\n","        # Write the same message to the \"Output/{cid}_Evaluation.txt\" file\n","        with open(f\"Output/{sub_dir_name}/{test_directory_name}/{self.cid}_Evaluation.txt\", \"a\") as f:\n","            f.write(f\"{datetime.datetime.now()} - Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}\\n\")\n","\n","            # Close the file\n","            f.close()\n","\n","        return loss, len(self.train_data), {\"accuracy\": accuracy}\n","\n","    def predict(self, incoming):\n","        prediction = np.argmax( self.model.predict(incoming) ,axis=1)\n","        return prediction\n","\n","def client_fn(cid: str) -> NumpyFlowerClient:\n","    \"\"\"Create a Flower client representing a single organization.\"\"\"\n","\n","    # Load model\n","    #model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n","    #model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","\n","    print (\"Client ID:\", cid)\n","\n","    model = Sequential([\n","      #Flatten(input_shape=(79,1)),\n","      Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n","      Dense(50, activation='relu'),\n","      Dense(25, activation='relu'),\n","      Dense(num_unique_classes, activation='softmax')\n","    ])\n","\n","    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","    partition_id = int(cid)\n","    X_train_c = fl_X_train[partition_id]\n","    y_train_c = fl_y_train[partition_id]\n","\n","    # Create a  single Flower client representing a single organization\n","    return NumpyFlowerClient(cid, model, X_train_c, y_train_c)\n","\n","\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","eval_count = 0\n","\n","def get_evaluate_fn(server_model):\n","    global eval_count\n","    \"\"\"Return an evaluation function for server-side evaluation.\"\"\"\n","    # The `evaluate` function will be called after every round\n","\n","\n","    def evaluate(server_round, parameters, config):\n","        global eval_count\n","\n","        # Update model with the latest parameters\n","        server_model.set_weights(parameters)\n","        print (f\"Server Evaluating... Evaluation Count:{eval_count}\")\n","        loss, accuracy = server_model.evaluate(X_test, y_test)\n","\n","        y_pred = server_model.predict(X_test)\n","        print (\"Prediction: \", y_pred, y_pred.shape)\n","        #cmatrix = confusion_matrix(y_test, np.rint(y_pred))\n","        #print (\"confusion_matrix:\", cmatrix, cmatrix.shape)\n","\n","        print(f\"{Colours.YELLOW.value}Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}{Colours.NORMAL.value}\")\n","\n","        # Write the same message to the \"Output/Server_Evaluation.txt\" file\n","        with open(f\"Output/{sub_dir_name}/{test_directory_name}/Server_Evaluation.txt\", \"a\") as f:\n","            f.write(f\"{datetime.datetime.now()} - {server_round} : Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\\n\")\n","\n","            # Close the file\n","            f.close()\n","\n","        np.save(\"y_pred-\" + str(eval_count) + \".npy\", y_pred)\n","        #np.save(\"cmatrix-\" + str(eval_count) + \".npy\", cmatrix)\n","        eval_count = eval_count + 1\n","\n","        return loss, {\"accuracy\": accuracy}\n","    return evaluate\n","\n","\n","\n","server_model = Sequential([\n","    #Flatten(input_shape=(79,1)),\n","    Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n","    Dense(50, activation='relu'),\n","    Dense(25, activation='relu'),\n","    Dense(num_unique_classes, activation='softmax')\n","])\n","\n","\n","server_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Create FedAvg strategy\n","strategy = fl.server.strategy.FedAvg(\n","        fraction_fit=1.0,\n","        fraction_evaluate=0.5,\n","        min_fit_clients=2, #10,\n","        min_evaluate_clients=2, #5,\n","        min_available_clients=2, #10,\n","        evaluate_fn=get_evaluate_fn(server_model),\n","        #evaluate_metrics_aggregation_fn=weighted_average,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YRydQIJI5ZX0","executionInfo":{"status":"aborted","timestamp":1749898324121,"user_tz":-420,"elapsed":735052,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oXmJ2L7I5ZX1","executionInfo":{"status":"aborted","timestamp":1749898324123,"user_tz":-420,"elapsed":734989,"user":{"displayName":"NGUYỄN ĐỨC THẮNG","userId":"15636327107964122138"}}},"outputs":[],"source":["%%time\n","print (f\"{Colours.YELLOW.value}\\nDeploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n","print (f\"Number of Clients = {NUM_OF_CLIENTS}\\n\")\n","print (f\"Writing output to: {sub_dir_name}/{test_directory_name}\\n{Colours.NORMAL.value}\")\n","\n","# Output the same information to the Output/Run_details.txt file\n","with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n","    f.write(f\"{datetime.datetime.now()} - Deploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n","    f.write(f\"{datetime.datetime.now()} - Number of Clients = {NUM_OF_CLIENTS}\\n\")\n","\n","    # Write Original train_df size\n","    f.write(f\"{datetime.datetime.now()} - Original train_df size: {train_df_shape}\\n\")\n","\n","    # Write the training data split groups\n","    for i in range(len(fl_X_train)):\n","        f.write(f\"{datetime.datetime.now()} - {i}: X Shape {fl_X_train[i].shape}, Y Shape {fl_y_train[i].shape}\\n\")\n","\n","    # Write the testing data\n","    f.write(f\"{datetime.datetime.now()} - X_test size: {X_test.shape}\\n\")\n","    f.write(f\"{datetime.datetime.now()} - y_test size: {y_test.shape}\\n\")\n","\n","# close the file\n","f.close()\n","\n","start_time = datetime.datetime.now()\n","\n","# Start simulation\n","fl.simulation.start_simulation(\n","    client_fn=client_fn,\n","    num_clients=NUM_OF_CLIENTS,\n","    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),\n","    strategy=strategy,\n",")\n","\n","end_time = datetime.datetime.now()\n","print(\"Total time taken: \", end_time - start_time)\n","\n","print (f\"{Colours.YELLOW.value} SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n","print (f\"Number of Clients = {NUM_OF_CLIENTS}{Colours.NORMAL.value}\\n\")\n","\n","# Output the same information to the Output/Run_details.txt file\n","with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n","    f.write(f\"{datetime.datetime.now()} - SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n","    f.write(f\"{datetime.datetime.now()} - Total time taken: {end_time - start_time}\\n\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}